---
title: 'Fundamental 1: Loss function'
date: 2024-03-29
categories: [Data Science, Deep Learning]
tags: [fundamental]
toc: true
math: true
publish: true
---
Blog n√†y s·∫Ω gi·ªõi thi·ªáu v·ªÅ c√°c h√†m loss (error function ho·∫∑c loss function) ƒë∆∞·ª£c s·ª≠ d·ª•ng trong Machine Learning & Deep Learning. H√†m loss tr√™n ƒë·ªùi th√¨ nhi·ªÅu v√¥ k·ªÉ, v·ªõi m√¨nh, h√†m loss ch·ªâ c·∫ßn ƒë·∫°t ƒë·ªß ti√™u ch√≠ ƒë·ªÉ n√≥ tr·ªü th√†nh m·ªôt h√†m loss, th√¨ n√≥ s·∫Ω l√† m·ªôt h√†m loss. Ok, m·ªùi m·ªçi ng∆∞·ªùi ƒë·ªçc

# 0. Danh m·ª•c:
Trong tr∆∞·ªùng h·ª£p b√†i vi·∫øt n√†y tr·ªü n√™n qu√° d√†i (v√† table of content kh√¥ng ho·∫°t ƒë·ªông, sr ü•π)
<!-- {% include toc %} -->
# 1. Gi·ªõi thi·ªáu
**TLDR**: H√†m loss l√† m·ªôt h√†m to√°n h·ªçc gi√∫p h·ªó tr·ª£ ƒëo l∆∞·ªùng hi·ªáu qu·∫£ ho·∫°t ƒë·ªông gi·ªØa d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh v√† gi√° tr·ªã th·ª±c t·∫ø (c√≥ nhi·ªÅu th∆∞·ªõc ƒëo hi·ªáu qu·∫£ ho·∫°t ƒë·ªông kh√°c nhau nh∆∞ l√† th·ªùi gian inference, ho·∫∑c l√† n·∫øu m√† x√©t t·ªõi c√°c services th√¨ s·∫Ω l√† th·ªùi gian h·ªìi ph·ª•c sau s·ª± c·ªë, v.v... N√≥i chung l√† t√πy v√†o objective c·ªßa ng∆∞·ªùi thi·∫øt k·∫ø). 

X√©t trong lƒ©nh v·ª±c h·ªçc m√°y, h√†m loss **ho·∫°t ƒë·ªông nh∆∞ l√† m·ªôt kim ch·ªâ nam** gi√∫p ƒëi·ªÅu h∆∞·ªõng qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh ƒë·∫°t t·ªõi **k·∫øt qu·∫£ t·ªëi ∆∞u cho c√°c objective c·ªßa m√¥ h√¨nh** (Objective m√¨nh kh√¥ng bi·∫øt d·ªãch ra ti·∫øng vi·ªát l√† sao, nh∆∞ng m√† c√≥ th·ªÉ hi·ªÉu n√≥ nh∆∞ l√† m·ª•c ti√™u m√¨nh nh·∫Øm t·ªõi, l·∫•y v√≠ d·ª• nh∆∞ m·∫•y b·∫°n mu·ªën sai s·ªë d·ª± ƒëo√°n l√† nh·ªè nh·∫•t, l√∫c n√†y objective c·ªßa m·ªçi ng∆∞·ªùi, c√°i m·ª•c ti√™u c·ªßa m·ªçi ng∆∞·ªùi l√† gi√° tr·ªã m·∫•t m√°t ƒë√≥ tr·ªü n√™n nh·ªè nh·∫•t, ho·∫∑c hi·ªÉu theo c√°ch kh√°c l√† m√¥ h√¨nh d·ª± ƒëo√°n ƒë√∫ng nh·∫•t). V√† do ƒë√≥, m·ªôt h√†m loss s·∫Ω mang c√°c t√≠nh ch·∫•t sau: 

1. **ƒêo l∆∞·ªùng hi·ªáu qu·∫£ ho·∫°t ƒë·ªông**: ƒê√¢y l√† m·ªôt ti√™u ch√≠ th·∫•y r√µ khi m√† ch√∫ng ta c√¢n nh·∫Øc hi·ªáu qu·∫£ ho·∫°t ƒë·ªông c·ªßa c√°c m√¥ h√¨nh, trong qu√° tr√¨nh h·ªçc t·∫≠p, ch√∫ng ta mong mu·ªën m√¥ h√¨nh ph·∫£i c√≥ loss c√†ng nh·ªè c√†ng t·ªët. N√≥ gi·ªëng nh∆∞ c√°c b·∫°n ƒëi l√†m b√†i ki·ªÉm tra √°, h·∫ßu h·∫øt b·∫°n n√†o c≈©ng mu·ªën m√¨nh ƒë∆∞·ª£c ƒëi·ªÉm cao, ƒë·ªìng nghƒ©a v·ªõi vi·ªác c√°c b·∫°n l√†m sai √≠t ƒëi, m√† ƒë·ªÉ c√°c b·∫°n l√†m sai √≠t ƒëi, th·∫≠m ch√≠ kh√¥ng sai c√†ng t·ªët, th√¨ c√¢u tr·∫£ l·ªùi c·ªßa c√°c b·∫°n (g·ªçi l√† prediction) v√† c√¢u tr·∫£ l·ªùi tr√™n c√°i ƒë√°p √°n (ground truth ho·∫∑c l√† label) n√≥ ph·∫£i c√†ng s√°t c√†ng t·ªët, n√≥ ƒë√∫ng lu√¥n c√†ng vui. 

2. **Kim ch·ªâ nam**: L·∫•y ti·∫øp c√°i v√≠ d·ª• ·ªü tr√™n, gi·ªù v√≠ d·ª• c√°c b·∫°n ƒë√©o h·ªçc g√¨ h·∫øt √°, th√¨ ƒëi·ªÉm c√°c b·∫°n sao cao ƒë∆∞·ª£c, n√™n l√† l√∫c m√† c√≥ ƒë√°p √°n, l√∫c m√† ƒë·ªëi chi·∫øu c√¢u tr·∫£ l·ªùi c·ªßa c√°c b·∫°n v·ªõi ƒë√°p √°n, **c√°c b·∫°n bi·∫øt m√¨nh sai ·ªü ƒë√¢u, c·∫ßn s·ª≠a ·ªü ƒë√¢u** th√¨ c√°c b·∫°n s·∫Ω s·ª≠a ·ªü ƒë√≥ (qu√° tr√¨nh m√† c√°c b·∫°n s·ª≠a nh∆∞ th·∫ø n√†o th√¨ ƒë√≥ l√† m·ªôt c√¢u chuy·ªán kh√°c, ch·∫Øc l√† s·∫Ω l√†m m·ªôt blog kh√°c). T∆∞∆°ng t·ª± cho c√°c b√†i to√°n h·ªçc m√°y, m√¥ h√¨nh sau khi d·ª± ƒëo√°n sai th√¨ s·∫Ω bi·∫øt n√≥ sai ·ªü ch·ªó n√†o ƒë·ªÉ m√† s·ª≠a cho k·∫øt qu·∫£ t·ªët h∆°n. 

3. **ƒêi·ªÅu ch·ªânh h√†nh vi**: L·∫•y ti·∫øp c√°i v√≠ d·ª• ·ªü tr√™n, gi·ªù v√¥ ph√≤ng thi, l·∫•y v√≠ d·ª• thi b·∫±ng l√°i ƒëi ha, n√≥ c√≥ m·∫•y c√¢u ƒëi·ªÉm li·ªát, m·∫•y b·∫°n l·ªách pha m·∫•y c√¢u ƒë√≥ l√† m·∫•y b·∫°n li·ªát lu√¥n, n√™n l√† trong tr∆∞·ªùng h·ª£p c·ªßa c√°c b·∫°n, **c√≥ nh·ªØng l·ªói sai n·∫∑ng n·ªÅ h∆°n nh·ªØng l·ªói sai kh√°c**, do ƒë√≥ c√°c b·∫°n **ph·∫£i x·ª≠ l√Ω ƒë·ªÉ kh√¥ng sai c√°c l·ªói ch√≠ t·ª≠**, t·ª©c l√† l√∫c n√†y, **objective c·ªßa c√°c b·∫°n s·∫Ω c√≥ s·ª± ƒëi·ªÅu ch·ªânh, h√†m loss c·ªßa c√°c b·∫°n s·∫Ω gi√∫p ƒëi·ªÅu ch·ªânh**. 

4. **C√¢n b·∫±ng**: C√¢n b·∫±ng ·ªü ƒë√¢y c√≥ nghƒ©a l√† **c√¢n b·∫±ng gi·ªØa *bias* v√† *variance*, cho ph√©p m√¥ h√¨nh t·ªïng qu√°t h∆°n.** Quay l·∫°i c√°i v√≠ d·ª• ·ªü tr√™n, m·ªôt h·ªçc sinh t·ªët l√† m·ªôt h·ªçc sinh **kh√¥ng h·ªçc v·∫πt** v√† **kh√¥ng h·ªçc ngu**. H·ªçc v·∫πt ·ªü ƒë√¢y v√≠ d·ª• nh∆∞ c√°c b·∫°n g·∫∑p r·ªìi c√°c b·∫°n m·ªõi l√†m ƒë∆∞·ª£c, c√≤n c√°c b·∫°n kh√¥ng g·∫∑p l√† c√°c b·∫°n t·ªãt lu√¥n (d√π l√† c√πng 1 d·∫°ng b√†i, ƒë·ªïi s·ªë th√¥i). H·ªçc ngu ·ªü ƒë√¢y l√† c√°c b·∫°n ch·∫£ hi·ªÉu g√¨, v√¥ c√°c b·∫°n ch·ªçn to√†n C (do c√°c b·∫°n th·∫•y c√°i n√†y c√≥ x√°c su·∫•t ƒë√∫ng cao, ch·ª© c√°c b·∫°n kh√¥ng hi·ªÉu sao n√≥ cao v·∫≠y), l√∫c n√†y c√°c b·∫°n c≈©ng c√∫c. 


M·ªôt h√†m loss c∆° b·∫£n theo m√¨nh c·∫ßn ƒë√°p ·ª©ng 2 ti√™u ch√≠ ƒë·∫ßu, m·ªôt h√†m loss t·ªët s·∫Ω ƒë√°p ·ª©ng t·ªët 4 ti√™u ch√≠. C·ª• th·ªÉ h∆°n c√≥ th·ªÉ xem c√°i h√¨nh d∆∞·ªõi ƒë√¢y: 

![optim](/assets/img/fund1/optim.png)

Tr√™n c√°i tr·ª•c loss, ·ªü gi√° tr·ªã c√†ng cao, c√†ng t·ªá (h·∫ßu h·∫øt l√† v·∫≠y), do ƒë√≥ m√† ·ªü l√∫c m·ªõi ƒë·∫ßu, c√≥ th·ªÉ n√≥i **hi·ªáu qu·∫£ ho·∫°t ƒë·ªông m√¥ h√¨nh ch∆∞a t·ªët**, do ƒë√≥ m√† m√¨nh c·∫ßn ƒëi·ªÅu ch·ªânh. Nh∆∞ng c√¢u h·ªèi l√† ph·∫£i ƒëi theo h∆∞·ªõng n√†o b√¢y gi·ªù, v√¨ kh√¥ng ph·∫£i c·ª© mu·ªën n√≥ ƒëi l√† n√≥ ƒëi, do ƒë√≥ v·ªõi m·ªôt h√†m loss ƒë∆∞·ª£c thi·∫øt k·∫ø, m√¥ h√¨nh s·∫Ω ƒëi·ªÅu ch·ªânh tham s·ªë sao cho n√≥ **ƒëi theo h∆∞·ªõng l√†m loss nh·ªè h∆°n**. ƒê√≥, c·ª© l√†m nh∆∞ v·∫≠y ƒë·ªß nhi·ªÅu l√† ch√∫ng ta s·∫Ω c√≥ m·ªôt m√¥ h√¨nh t·ªët (hy v·ªçng v·∫≠y). Nh∆∞ng h√†m loss t·ªët, s·∫Ω gi√∫p c√°c b·∫°n xu·ªëng ƒëi·ªÉm loss ·ªü t·∫≠n c√πng lu√¥n, c√≤n m·ªôt h√†m loss ch∆∞a t·ªët th√¨ ch∆∞a l√†m ƒë∆∞·ª£c vi·ªác ƒë√≥, quay tr·ªü l·∫°i v·ªõi √Ω "h∆∞·ªõng l√†m h√†m loss nh·ªè h∆°n", **c√≥ nhi·ªÅu h∆∞·ªõng l√†m gi√° tr·ªã loss nh·ªè h∆°n, nh∆∞ng m·ªôt h√†m loss t·ªët s·∫Ω ƒë∆∞a ra h∆∞·ªõng l√†m gi√° tr·ªã loss nh·ªè nh·∫•t**. C∆° b·∫£n l√† v·∫≠y ha. 

# 2. C√°c h√†m loss ph·ªï bi·∫øn
Trong h·ªçc m√°y, c√°c h√†m loss th∆∞·ªùng ƒë∆∞·ª£c x·∫Øp s·∫øp theo c√°c b√†i to√°n m√† n√≥ ƒëang gi·∫£i quy·∫øt, do ƒë√≥ m√† h·∫ßu h·∫øt n√≥ r∆°i v√†o 2 d·∫°ng to√°n l√† h·ªìi quy ho·∫∑c ph√¢n lo·∫°i (c√°c tr∆∞·ªùng h·ª£p ƒë∆°n gi·∫£n). ·ªû m·ª©c c∆° b·∫£n, c√°c b√†i to√°n h·ªìi quy th∆∞·ªùng cho ra gi√° tr·ªã d·ª± ƒëo√°n l√† m·ªôt s·ªë th·ª±c, trong khi ƒë√≥ c√°c b√†i to√°n ph√¢n lo·∫°i th√¨ cho ra gi√° tr·ªã l√† c√°c nh√£n r·ªùi r·∫°c. 

## 2.1 Ti√™u ch√≠ ch·ªçn h√†m loss
**TLDR**: H√†m loss m√† b·∫°n ch·ªçn ph·∫£i c√πng ch√≠ h∆∞·ªõng v·ªõi objective m√† b·∫°n mu·ªën n√≥ gi√∫p b·∫°n ƒë·∫°t ƒë∆∞·ª£c. Ngo√†i ra kh√¥ng ph·∫£i c·ª© mu·ªën ch·ªçn h√†m loss n√†o c≈©ng ƒë∆∞·ª£c, s·∫Ω c√≥ nhi·ªÅu h√†m loss ·∫£nh h∆∞·ªüng b·ªüi outlier, c√≥ nhi·ªÅu h√†m kh√°c th√¨ l·∫°i kh√¥ng ch·ªãu ·∫£nh h∆∞·ªüng ƒë√≥. 

C√°c b·∫°n ph·∫£i ƒë·ªÉ √Ω t·ªõi c√¥ng th·ª©c to√°n h·ªçc c≈©ng nh∆∞ l√† √Ω nghƒ©a ƒë·∫±ng sau nh·ªØng c√¥ng th·ª©c to√°n ƒë√≥, mang n√≥ so s√°nh v·ªõi c√°c task c·ªßa b·∫°n, gi·ªù l·∫•y v√≠ d·ª• nh∆∞ m√¨nh mang m·ªôt h√†m loss hay s·ª≠ d·ª•ng b√™n c√°c nhi·ªám v·ª• v·ªÅ h·ªìi quy, r·ªìi m√¨nh mang sang ƒë·ªÉ l√†m loss function cho b√†i to√°n ph√¢n lo·∫°i th√¨ l√∫c n√†y n√≥ ph·∫£n √°nh kh√¥ng c√≥ ƒë√∫ng n·ªØa. 

V√≠ d·ª• nh∆∞ trong m·ªôt b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n, h√†m loss s·ª≠ d·ª•ng th∆∞·ªùng l√† *Binary Cross Entropy* (s·∫Ω gi·ªõi thi·ªáu ph√≠a d∆∞·ªõi) m√† m√¨nh kh√¥ng d√πng, m√¨nh t·ª± nhi√™n d√πng *Mean Squared Error* (s·∫Ω gi·ªõi thi·ªáu ph√≠a d∆∞·ªõi) v·ªën l√† m·ªôt h√†m loss hay ƒëi chung v·ªõi nhi·ªám v·ª• h·ªìi quy. V·∫•n ƒë·ªÅ x·∫£y ra ·ªü ch·ªó l√† MSE s·∫Ω ƒë·ªëi x·ª≠ v·ªõi c√°c l·ªói ph√¢n lo·∫°i nh∆∞ nhau trong khi ta mu·ªën n·∫øu nh∆∞ l·ªói c√†ng l·ªõn th√¨ ph·∫°t c√†ng n·∫∑ng, MSE kh√¥ng l√†m chuy·ªán ƒë√≥. 

B√™n c·∫°nh ƒë√≥ c≈©ng c√≥ c√°c y·∫øu t·ªë kh√°c s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn vi·ªác ch√∫ng ta l·ª±a ch·ªçn h√†m loss, nh∆∞ l√† c√°c v·∫•n ƒë·ªÅ v·ªÅ t√≠nh h·ªôi t·ª• c≈©ng nh∆∞ l√† scale c·ªßa task. √Ä, v√† n√≥ c√≤n l√† v·∫•n ƒë·ªÅ v·ªÅ thu·∫≠t to√°n n·ªØa. V√≠ d·ª• nh∆∞ trong machine learning, ch√∫ng ta th∆∞·ªùng s·ª≠ d·ª•ng gradient-based approach ƒë·ªÉ c·∫≠p nh·∫≠t tr·ªçng s·ªë trong m√¥ h√¨nh do ƒë√≥ m√† y√™u c·∫ßu h√†m loss ph·∫£i l√† m·ªôt h√†m s·ªë kh·∫£ vi, nh∆∞ng v·ªõi c√°c thu·∫≠t to√°n heuristic approach, v√≠ d·ª• c·ª• th·ªÉ nh∆∞ l√† gi·∫£i thu·∫≠t di truy·ªÅn, ch√∫ng ta s·ª≠ d·ª•ng c√°c h√†m value function (idea t·ª±a t·ª±a h√†m loss, idea d·ªãch ng∆∞·ª£c l·∫°i t·ª´ h√†m loss, t·ª©c l√† ta mu·ªën maximize gi√° tr·ªã c·ªßa h√†m n√†y) r·ªìi sau ƒë√≥ s·ª≠ d·ª•ng evolutionary search (c√°c b∆∞·ªõc nh∆∞ ch·ªçn l·ªçc, r·ªìi lai t·∫°o, r·ªìi ƒë·ªôt bi·∫øn) ƒë·ªÉ h∆∞·ªõng t·ªõi k·∫øt qu·∫£ t·ªët h∆°n, v√† do ƒë√≥ value function kh√¥ng c·∫ßn ph·∫£i l√† m·ªôt h√†m kh·∫£ vi. 

M·ªôt v·∫•n ƒë·ªÅ kh√°c c≈©ng c·∫ßn ph·∫£i l∆∞u t√¢m ƒë√≥ ch√≠nh l√† ph√¢n ph·ªëi c·ªßa d·ªØ li·ªáu, v√≠ d·ª• nh∆∞ trong b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n, c·ª• th·ªÉ l√† trong tr∆∞·ªùng h·ª£p d·ªØ li·ªáu b·ªã m·∫•t c√¢n b·∫±ng (m·ªôt class c√≥ s·ªë l∆∞·ª£ng sample th·ªëng tr·ªã s·ªë l∆∞·ª£ng sample c·ªßa class c√≤n l·∫°i), th∆∞·ªùng g·∫∑p v·∫•n ƒë·ªÅ n√†y l√† trong c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn l·ª´a ƒë·∫£o t√≠n d·ª•ng, ho·∫∑c l√† d·ª± ƒëo√°n b·ªánh ung th∆∞. 2 c√°i n√†y l√† c√°i case kinh ƒëi·ªÉn trong c√°c b√†i to√°n c√≥ li√™n quan ƒë·∫øn vi·ªác d·ªØ li·ªáu b·ªã m·∫•t c√¢n b·∫±ng. Ho·∫∑c m·ªôt tr∆∞·ªùng h·ª£p kh√°c ƒë√≥ l√† ch√∫ng ta g·∫∑p ph·∫£i long-tail distribution, nh∆∞ l√† h√¨nh d∆∞·ªõi ƒë√¢y:

![long_tail](/assets/img/fund1/longtail.jpg){: .align-center}

ƒê√≥, ch·ª© kh√¥ng ph·∫£i mu·ªën ch·ªçn l√† ch·ªçn, m√† sau khi ch·ªçn r·ªìi ch√∫ng ta ph·∫£i nghƒ© xem ch√∫ng ta c·∫ßn ph·∫£i ch·ªçn metrics nh∆∞ th·∫ø n√†o cho n√≥ ph√π h·ª£p (v·ªën l√† m·ªôt ch·ªß ƒë·ªÅ kh√°c c≈©ng khoai kh√¥ng k·∫øm.) 

> Kh√¥ng ph·∫£i mu·ªën ch·ªçn l√† ch·ªçn, nghƒ© r·ªìi ch·ªçn.

## 2.2 C√°c h√†m loss cho b√†i to√°n ph√¢n lo·∫°i

Trong h·ªçc m√°y, c√°c b√†i to√°n ph√¢n lo·∫°i l√† b√†i to√°n g√°n cho c√°c ƒëi·ªÉm d·ªØ li·ªáu m·ªôt nh√£n c·ª• th·ªÉ (ho·∫∑c nhi·ªÅu nh√£n, t√πy b√†i to√°n). V·ªõi m·ª•c ti√™u c·ª• th·ªÉ nh∆∞ v·∫≠y, ch√∫ng ta mong mu·ªën gi√° tr·ªã tr·∫£ ra b·ªüi m√¥ h√¨nh l√† m·ªôt l√† m·ªôt ph√¢n ph·ªëi x√°c su·∫•t th·ªÉ hi·ªán kh·∫£ nƒÉng m√† m·ªôt ƒëi·ªÉm thu·ªôc v√†o m·ªôt l·ªõp b·∫•t k·ª≥ n√†o ƒë√≥. 

### 2.2.1 Cross-Entropy loss

### 2.2.2 Hinge Loss

### 2.2.3 Log Loss

### 2.2.4 Focal Loss
ƒê∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£i quy·∫øt c√°c b√†i to√°n ph√¢n lo·∫°i m√† ·ªü ƒë√≥ d·ªØ li·ªáu b·ªã m·∫•t c√¢n b·∫±ng

## 2.3 C√°c h√†m loss cho b√†i to√°n h·ªìi quy

·ªí, **ngo√†i ra m√¨nh th·∫•y h√†m n√†o ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√≠nh kho·∫£ng c√°ch ƒë·ªÅu c√≥ kh·∫£ nƒÉng l√† m·ªôt h√†m loss, h·ªØu d·ª•ng kh√¥ng th√¨ ch∆∞a c√≥ ch·∫Øc**
### 2.3.1 Mean Squared Error
Hay c√≤n ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† L2 Loss, l√† m·ªôt h√†m loss r·∫•t hay g·∫∑p, ƒë∆°n gi·∫£n, d·ªÖ d√πng, hi·ªáu qu·∫£, song v·∫´n c√≥ nh∆∞·ª£c ƒëi·ªÉm (ƒë·ªÅ c·∫≠p sau), d∆∞·ªõi ƒë√¢y l√† c√¥ng th·ª©c: 

$$
\begin{equation}
L(y, y') = \frac{1}{n} \sum_{i=1}^{n} (y_i - y'_i)^2
\end{equation}
$$

Trong ƒë√≥:

- $n$ l√† s·ªë l∆∞·ª£ng sample trong b·ªô d·ªØ li·ªáu (ho·∫∑c batch d·ªØ li·ªáu)
- $y_i$ l√† gi√° tr·ªã d·ª± ƒëo√°n c·ªßa m·∫´u th·ª© $i$
- $y'$ l√† gi√° tr·ªã th·ª±c c·ªßa m·∫´u th·ª© $i$

H√†m loss n√†y s·∫Ω ho·∫°t ƒë·ªông v√¥ c√πng t·ªët n·∫øu nh∆∞ ph√¢n ph·ªëi c·ªßa d·ªØ li·ªáu tu√¢n theo ph√¢n ph·ªëi chu·∫©n, v√† kh√¥ng c√≥ outlier. 
### 2.3.2 Huber Loss 

## 2.4 C√°c h√†m loss kh√°c

### 2.4.1 Kullback-Leibler divergence Loss (KL Loss)

### 2.4.2 Earth Mover's Distance (EMD)