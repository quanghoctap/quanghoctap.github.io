---
title: 'Fundamental 1: Loss function'
date: 2024-03-29
categories: [Data Science, Deep Learning]
tags: [fundamental]
toc: true
math: true
publish: true
---
Blog nÃ y sáº½ giá»›i thiá»‡u vá» cÃ¡c hÃ m loss (error function hoáº·c loss function) Ä‘Æ°á»£c sá»­ dá»¥ng trong Machine Learning & Deep Learning. HÃ m loss trÃªn Ä‘á»i thÃ¬ nhiá»u vÃ´ ká»ƒ, vá»›i mÃ¬nh, hÃ m loss chá»‰ cáº§n Ä‘áº¡t Ä‘á»§ tiÃªu chÃ­ Ä‘á»ƒ nÃ³ trá»Ÿ thÃ nh má»™t hÃ m loss, thÃ¬ nÃ³ sáº½ lÃ  má»™t hÃ m loss. Ok, má»i má»i ngÆ°á»i Ä‘á»c

# 0. Danh má»¥c:
Trong trÆ°á»ng há»£p bÃ i viáº¿t nÃ y trá»Ÿ nÃªn quÃ¡ dÃ i (vÃ  table of content khÃ´ng hoáº¡t Ä‘á»™ng, sr ğŸ¥¹)
<!-- {% include toc %} -->
# 1. Giá»›i thiá»‡u
**TLDR**: HÃ m loss lÃ  má»™t hÃ m toÃ¡n há»c giÃºp há»— trá»£ Ä‘o lÆ°á»ng hiá»‡u quáº£ hoáº¡t Ä‘á»™ng giá»¯a dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vÃ  giÃ¡ trá»‹ thá»±c táº¿ (cÃ³ nhiá»u thÆ°á»›c Ä‘o hiá»‡u quáº£ hoáº¡t Ä‘á»™ng khÃ¡c nhau nhÆ° lÃ  thá»i gian inference, hoáº·c lÃ  náº¿u mÃ  xÃ©t tá»›i cÃ¡c services thÃ¬ sáº½ lÃ  thá»i gian há»“i phá»¥c sau sá»± cá»‘, v.v... NÃ³i chung lÃ  tÃ¹y vÃ o objective cá»§a ngÆ°á»i thiáº¿t káº¿). 

XÃ©t trong lÄ©nh vá»±c há»c mÃ¡y, hÃ m loss **hoáº¡t Ä‘á»™ng nhÆ° lÃ  má»™t kim chá»‰ nam** giÃºp Ä‘iá»u hÆ°á»›ng quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘áº¡t tá»›i **káº¿t quáº£ tá»‘i Æ°u cho cÃ¡c objective cá»§a mÃ´ hÃ¬nh** (Objective mÃ¬nh khÃ´ng biáº¿t dá»‹ch ra tiáº¿ng viá»‡t lÃ  sao, nhÆ°ng mÃ  cÃ³ thá»ƒ hiá»ƒu nÃ³ nhÆ° lÃ  má»¥c tiÃªu mÃ¬nh nháº¯m tá»›i, láº¥y vÃ­ dá»¥ nhÆ° máº¥y báº¡n muá»‘n sai sá»‘ dá»± Ä‘oÃ¡n lÃ  nhá» nháº¥t, lÃºc nÃ y objective cá»§a má»i ngÆ°á»i, cÃ¡i má»¥c tiÃªu cá»§a má»i ngÆ°á»i lÃ  giÃ¡ trá»‹ máº¥t mÃ¡t Ä‘Ã³ trá»Ÿ nÃªn nhá» nháº¥t, hoáº·c hiá»ƒu theo cÃ¡ch khÃ¡c lÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng nháº¥t). VÃ  do Ä‘Ã³, má»™t hÃ m loss sáº½ mang cÃ¡c tÃ­nh cháº¥t sau: 

1. **Äo lÆ°á»ng hiá»‡u quáº£ hoáº¡t Ä‘á»™ng**: ÄÃ¢y lÃ  má»™t tiÃªu chÃ­ tháº¥y rÃµ khi mÃ  chÃºng ta cÃ¢n nháº¯c hiá»‡u quáº£ hoáº¡t Ä‘á»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh, trong quÃ¡ trÃ¬nh há»c táº­p, chÃºng ta mong muá»‘n mÃ´ hÃ¬nh pháº£i cÃ³ loss cÃ ng nhá» cÃ ng tá»‘t. NÃ³ giá»‘ng nhÆ° cÃ¡c báº¡n Ä‘i lÃ m bÃ i kiá»ƒm tra Ã¡, háº§u háº¿t báº¡n nÃ o cÅ©ng muá»‘n mÃ¬nh Ä‘Æ°á»£c Ä‘iá»ƒm cao, Ä‘á»“ng nghÄ©a vá»›i viá»‡c cÃ¡c báº¡n lÃ m sai Ã­t Ä‘i, mÃ  Ä‘á»ƒ cÃ¡c báº¡n lÃ m sai Ã­t Ä‘i, tháº­m chÃ­ khÃ´ng sai cÃ ng tá»‘t, thÃ¬ cÃ¢u tráº£ lá»i cá»§a cÃ¡c báº¡n (gá»i lÃ  prediction) vÃ  cÃ¢u tráº£ lá»i trÃªn cÃ¡i Ä‘Ã¡p Ã¡n (ground truth hoáº·c lÃ  label) nÃ³ pháº£i cÃ ng sÃ¡t cÃ ng tá»‘t, nÃ³ Ä‘Ãºng luÃ´n cÃ ng vui. 

2. **Kim chá»‰ nam**: Láº¥y tiáº¿p cÃ¡i vÃ­ dá»¥ á»Ÿ trÃªn, giá» vÃ­ dá»¥ cÃ¡c báº¡n Ä‘Ã©o há»c gÃ¬ háº¿t Ã¡, thÃ¬ Ä‘iá»ƒm cÃ¡c báº¡n sao cao Ä‘Æ°á»£c, nÃªn lÃ  lÃºc mÃ  cÃ³ Ä‘Ã¡p Ã¡n, lÃºc mÃ  Ä‘á»‘i chiáº¿u cÃ¢u tráº£ lá»i cá»§a cÃ¡c báº¡n vá»›i Ä‘Ã¡p Ã¡n, **cÃ¡c báº¡n biáº¿t mÃ¬nh sai á»Ÿ Ä‘Ã¢u, cáº§n sá»­a á»Ÿ Ä‘Ã¢u** thÃ¬ cÃ¡c báº¡n sáº½ sá»­a á»Ÿ Ä‘Ã³ (quÃ¡ trÃ¬nh mÃ  cÃ¡c báº¡n sá»­a nhÆ° tháº¿ nÃ o thÃ¬ Ä‘Ã³ lÃ  má»™t cÃ¢u chuyá»‡n khÃ¡c, cháº¯c lÃ  sáº½ lÃ m má»™t blog khÃ¡c). TÆ°Æ¡ng tá»± cho cÃ¡c bÃ i toÃ¡n há»c mÃ¡y, mÃ´ hÃ¬nh sau khi dá»± Ä‘oÃ¡n sai thÃ¬ sáº½ biáº¿t nÃ³ sai á»Ÿ chá»— nÃ o Ä‘á»ƒ mÃ  sá»­a cho káº¿t quáº£ tá»‘t hÆ¡n. 

3. **Äiá»u chá»‰nh hÃ nh vi**: Láº¥y tiáº¿p cÃ¡i vÃ­ dá»¥ á»Ÿ trÃªn, giá» vÃ´ phÃ²ng thi, láº¥y vÃ­ dá»¥ thi báº±ng lÃ¡i Ä‘i ha, nÃ³ cÃ³ máº¥y cÃ¢u Ä‘iá»ƒm liá»‡t, máº¥y báº¡n lá»‡ch pha máº¥y cÃ¢u Ä‘Ã³ lÃ  máº¥y báº¡n liá»‡t luÃ´n, nÃªn lÃ  trong trÆ°á»ng há»£p cá»§a cÃ¡c báº¡n, **cÃ³ nhá»¯ng lá»—i sai náº·ng ná» hÆ¡n nhá»¯ng lá»—i sai khÃ¡c**, do Ä‘Ã³ cÃ¡c báº¡n **pháº£i xá»­ lÃ½ Ä‘á»ƒ khÃ´ng sai cÃ¡c lá»—i chÃ­ tá»­**, tá»©c lÃ  lÃºc nÃ y, **objective cá»§a cÃ¡c báº¡n sáº½ cÃ³ sá»± Ä‘iá»u chá»‰nh, hÃ m loss cá»§a cÃ¡c báº¡n sáº½ giÃºp Ä‘iá»u chá»‰nh**. 

4. **CÃ¢n báº±ng**: CÃ¢n báº±ng á»Ÿ Ä‘Ã¢y cÃ³ nghÄ©a lÃ  **cÃ¢n báº±ng giá»¯a *bias* vÃ  *variance*, cho phÃ©p mÃ´ hÃ¬nh tá»•ng quÃ¡t hÆ¡n.** Quay láº¡i cÃ¡i vÃ­ dá»¥ á»Ÿ trÃªn, má»™t há»c sinh tá»‘t lÃ  má»™t há»c sinh **khÃ´ng há»c váº¹t** vÃ  **khÃ´ng há»c ngu**. Há»c váº¹t á»Ÿ Ä‘Ã¢y vÃ­ dá»¥ nhÆ° cÃ¡c báº¡n gáº·p rá»“i cÃ¡c báº¡n má»›i lÃ m Ä‘Æ°á»£c, cÃ²n cÃ¡c báº¡n khÃ´ng gáº·p lÃ  cÃ¡c báº¡n tá»‹t luÃ´n (dÃ¹ lÃ  cÃ¹ng 1 dáº¡ng bÃ i, Ä‘á»•i sá»‘ thÃ´i). Há»c ngu á»Ÿ Ä‘Ã¢y lÃ  cÃ¡c báº¡n cháº£ hiá»ƒu gÃ¬, vÃ´ cÃ¡c báº¡n chá»n toÃ n C (do cÃ¡c báº¡n tháº¥y cÃ¡i nÃ y cÃ³ xÃ¡c suáº¥t Ä‘Ãºng cao, chá»© cÃ¡c báº¡n khÃ´ng hiá»ƒu sao nÃ³ cao váº­y), lÃºc nÃ y cÃ¡c báº¡n cÅ©ng cÃºc. 


Má»™t hÃ m loss cÆ¡ báº£n theo mÃ¬nh cáº§n Ä‘Ã¡p á»©ng 2 tiÃªu chÃ­ Ä‘áº§u, má»™t hÃ m loss tá»‘t sáº½ Ä‘Ã¡p á»©ng tá»‘t 4 tiÃªu chÃ­. Cá»¥ thá»ƒ hÆ¡n cÃ³ thá»ƒ xem cÃ¡i hÃ¬nh dÆ°á»›i Ä‘Ã¢y: 

![optim](/assets/img/fund1/optim.png)

TrÃªn cÃ¡i trá»¥c loss, á»Ÿ giÃ¡ trá»‹ cÃ ng cao, cÃ ng tá»‡ (háº§u háº¿t lÃ  váº­y), do Ä‘Ã³ mÃ  á»Ÿ lÃºc má»›i Ä‘áº§u, cÃ³ thá»ƒ nÃ³i **hiá»‡u quáº£ hoáº¡t Ä‘á»™ng mÃ´ hÃ¬nh chÆ°a tá»‘t**, do Ä‘Ã³ mÃ  mÃ¬nh cáº§n Ä‘iá»u chá»‰nh. NhÆ°ng cÃ¢u há»i lÃ  pháº£i Ä‘i theo hÆ°á»›ng nÃ o bÃ¢y giá», vÃ¬ khÃ´ng pháº£i cá»© muá»‘n nÃ³ Ä‘i lÃ  nÃ³ Ä‘i, do Ä‘Ã³ vá»›i má»™t hÃ m loss Ä‘Æ°á»£c thiáº¿t káº¿, mÃ´ hÃ¬nh sáº½ Ä‘iá»u chá»‰nh tham sá»‘ sao cho nÃ³ **Ä‘i theo hÆ°á»›ng lÃ m loss nhá» hÆ¡n**. ÄÃ³, cá»© lÃ m nhÆ° váº­y Ä‘á»§ nhiá»u lÃ  chÃºng ta sáº½ cÃ³ má»™t mÃ´ hÃ¬nh tá»‘t (hy vá»ng váº­y). NhÆ°ng hÃ m loss tá»‘t, sáº½ giÃºp cÃ¡c báº¡n xuá»‘ng Ä‘iá»ƒm loss á»Ÿ táº­n cÃ¹ng luÃ´n, cÃ²n má»™t hÃ m loss chÆ°a tá»‘t thÃ¬ chÆ°a lÃ m Ä‘Æ°á»£c viá»‡c Ä‘Ã³, quay trá»Ÿ láº¡i vá»›i Ã½ "hÆ°á»›ng lÃ m hÃ m loss nhá» hÆ¡n", **cÃ³ nhiá»u hÆ°á»›ng lÃ m giÃ¡ trá»‹ loss nhá» hÆ¡n, nhÆ°ng má»™t hÃ m loss tá»‘t sáº½ Ä‘Æ°a ra hÆ°á»›ng lÃ m giÃ¡ trá»‹ loss nhá» nháº¥t**. CÆ¡ báº£n lÃ  váº­y ha. 

# 2. CÃ¡c hÃ m loss phá»• biáº¿n
Trong há»c mÃ¡y, cÃ¡c hÃ m loss thÆ°á»ng Ä‘Æ°á»£c xáº¯p sáº¿p theo cÃ¡c bÃ i toÃ¡n mÃ  nÃ³ Ä‘ang giáº£i quyáº¿t, do Ä‘Ã³ mÃ  háº§u háº¿t nÃ³ rÆ¡i vÃ o 2 dáº¡ng toÃ¡n lÃ  há»“i quy hoáº·c phÃ¢n loáº¡i (cÃ¡c trÆ°á»ng há»£p Ä‘Æ¡n giáº£n). á» má»©c cÆ¡ báº£n, cÃ¡c bÃ i toÃ¡n há»“i quy thÆ°á»ng cho ra giÃ¡ trá»‹ dá»± Ä‘oÃ¡n lÃ  má»™t sá»‘ thá»±c, trong khi Ä‘Ã³ cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i thÃ¬ cho ra giÃ¡ trá»‹ lÃ  cÃ¡c nhÃ£n rá»i ráº¡c. 

## 2.1 TiÃªu chÃ­ chá»n hÃ m loss
**TLDR**: HÃ m loss mÃ  báº¡n chá»n pháº£i cÃ¹ng chÃ­ hÆ°á»›ng vá»›i objective mÃ  báº¡n muá»‘n nÃ³ giÃºp báº¡n Ä‘áº¡t Ä‘Æ°á»£c. NgoÃ i ra khÃ´ng pháº£i cá»© muá»‘n chá»n hÃ m loss nÃ o cÅ©ng Ä‘Æ°á»£c, sáº½ cÃ³ nhiá»u hÃ m loss áº£nh hÆ°á»Ÿng bá»Ÿi outlier, cÃ³ nhiá»u hÃ m khÃ¡c thÃ¬ láº¡i khÃ´ng chá»‹u áº£nh hÆ°á»Ÿng Ä‘Ã³. 

CÃ¡c báº¡n pháº£i Ä‘á»ƒ Ã½ tá»›i cÃ´ng thá»©c toÃ¡n há»c cÅ©ng nhÆ° lÃ  Ã½ nghÄ©a Ä‘áº±ng sau nhá»¯ng cÃ´ng thá»©c toÃ¡n Ä‘Ã³, mang nÃ³ so sÃ¡nh vá»›i cÃ¡c task cá»§a báº¡n, giá» láº¥y vÃ­ dá»¥ nhÆ° mÃ¬nh mang má»™t hÃ m loss hay sá»­ dá»¥ng bÃªn cÃ¡c nhiá»‡m vá»¥ vá» há»“i quy, rá»“i mÃ¬nh mang sang Ä‘á»ƒ lÃ m loss function cho bÃ i toÃ¡n phÃ¢n loáº¡i thÃ¬ lÃºc nÃ y nÃ³ pháº£n Ã¡nh khÃ´ng cÃ³ Ä‘Ãºng ná»¯a. 

VÃ­ dá»¥ nhÆ° trong má»™t bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n, hÃ m loss sá»­ dá»¥ng thÆ°á»ng lÃ  *Binary Cross Entropy* (sáº½ giá»›i thiá»‡u phÃ­a dÆ°á»›i) mÃ  mÃ¬nh khÃ´ng dÃ¹ng, mÃ¬nh tá»± nhiÃªn dÃ¹ng *Mean Squared Error* (sáº½ giá»›i thiá»‡u phÃ­a dÆ°á»›i) vá»‘n lÃ  má»™t hÃ m loss hay Ä‘i chung vá»›i nhiá»‡m vá»¥ há»“i quy. Váº¥n Ä‘á» xáº£y ra á»Ÿ chá»— lÃ  MSE sáº½ Ä‘á»‘i xá»­ vá»›i cÃ¡c lá»—i phÃ¢n loáº¡i nhÆ° nhau trong khi ta muá»‘n náº¿u nhÆ° lá»—i cÃ ng lá»›n thÃ¬ pháº¡t cÃ ng náº·ng, MSE khÃ´ng lÃ m chuyá»‡n Ä‘Ã³. 

BÃªn cáº¡nh Ä‘Ã³ cÅ©ng cÃ³ cÃ¡c yáº¿u tá»‘ khÃ¡c sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c chÃºng ta lá»±a chá»n hÃ m loss, nhÆ° lÃ  cÃ¡c váº¥n Ä‘á» vá» tÃ­nh há»™i tá»¥ cÅ©ng nhÆ° lÃ  scale cá»§a task. Ã€, vÃ  nÃ³ cÃ²n lÃ  váº¥n Ä‘á» vá» thuáº­t toÃ¡n ná»¯a. VÃ­ dá»¥ nhÆ° trong machine learning, chÃºng ta thÆ°á»ng sá»­ dá»¥ng gradient-based approach Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ trong mÃ´ hÃ¬nh do Ä‘Ã³ mÃ  yÃªu cáº§u hÃ m loss pháº£i lÃ  má»™t hÃ m sá»‘ kháº£ vi, nhÆ°ng vá»›i cÃ¡c thuáº­t toÃ¡n heuristic approach, vÃ­ dá»¥ cá»¥ thá»ƒ nhÆ° lÃ  giáº£i thuáº­t di truyá»n, chÃºng ta sá»­ dá»¥ng cÃ¡c hÃ m value function (idea tá»±a tá»±a hÃ m loss, idea dá»‹ch ngÆ°á»£c láº¡i tá»« hÃ m loss, tá»©c lÃ  ta muá»‘n maximize giÃ¡ trá»‹ cá»§a hÃ m nÃ y) rá»“i sau Ä‘Ã³ sá»­ dá»¥ng evolutionary search (cÃ¡c bÆ°á»›c nhÆ° chá»n lá»c, rá»“i lai táº¡o, rá»“i Ä‘á»™t biáº¿n) Ä‘á»ƒ hÆ°á»›ng tá»›i káº¿t quáº£ tá»‘t hÆ¡n, vÃ  do Ä‘Ã³ value function khÃ´ng cáº§n pháº£i lÃ  má»™t hÃ m kháº£ vi. 

Má»™t váº¥n Ä‘á» khÃ¡c cÅ©ng cáº§n pháº£i lÆ°u tÃ¢m Ä‘Ã³ chÃ­nh lÃ  phÃ¢n phá»‘i cá»§a dá»¯ liá»‡u, vÃ­ dá»¥ nhÆ° trong bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n, cá»¥ thá»ƒ lÃ  trong trÆ°á»ng há»£p dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng (má»™t class cÃ³ sá»‘ lÆ°á»£ng sample thá»‘ng trá»‹ sá»‘ lÆ°á»£ng sample cá»§a class cÃ²n láº¡i), thÆ°á»ng gáº·p váº¥n Ä‘á» nÃ y lÃ  trong cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n lá»«a Ä‘áº£o tÃ­n dá»¥ng, hoáº·c lÃ  dá»± Ä‘oÃ¡n bá»‡nh ung thÆ°. 2 cÃ¡i nÃ y lÃ  cÃ¡i case kinh Ä‘iá»ƒn trong cÃ¡c bÃ i toÃ¡n cÃ³ liÃªn quan Ä‘áº¿n viá»‡c dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng. Hoáº·c má»™t trÆ°á»ng há»£p khÃ¡c Ä‘Ã³ lÃ  chÃºng ta gáº·p pháº£i long-tail distribution, nhÆ° lÃ  hÃ¬nh dÆ°á»›i Ä‘Ã¢y:

![long_tail](/assets/img/fund1/longtail.jpg){: .align-center}

ÄÃ³, chá»© khÃ´ng pháº£i muá»‘n chá»n lÃ  chá»n, mÃ  sau khi chá»n rá»“i chÃºng ta pháº£i nghÄ© xem chÃºng ta cáº§n pháº£i chá»n metrics nhÆ° tháº¿ nÃ o cho nÃ³ phÃ¹ há»£p (vá»‘n lÃ  má»™t chá»§ Ä‘á» khÃ¡c cÅ©ng khoai khÃ´ng káº¿m.) 

> KhÃ´ng pháº£i muá»‘n chá»n lÃ  chá»n, nghÄ© rá»“i chá»n.

## 2.2 CÃ¡c hÃ m loss cho bÃ i toÃ¡n phÃ¢n loáº¡i

Trong há»c mÃ¡y, cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i lÃ  bÃ i toÃ¡n gÃ¡n cho cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u má»™t nhÃ£n cá»¥ thá»ƒ (hoáº·c nhiá»u nhÃ£n, tÃ¹y bÃ i toÃ¡n). Vá»›i má»¥c tiÃªu cá»¥ thá»ƒ nhÆ° váº­y, chÃºng ta mong muá»‘n giÃ¡ trá»‹ tráº£ ra bá»Ÿi mÃ´ hÃ¬nh lÃ  má»™t lÃ  má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t thá»ƒ hiá»‡n kháº£ nÄƒng mÃ  má»™t Ä‘iá»ƒm thuá»™c vÃ o má»™t lá»›p báº¥t ká»³ nÃ o Ä‘Ã³. 

### 2.2.1 Cross-Entropy loss

### 2.2.2 Hinge Loss

### 2.2.3 Log Loss

### 2.2.4 Focal Loss
ÄÆ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i mÃ  á»Ÿ Ä‘Ã³ dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng

## 2.3 CÃ¡c hÃ m loss cho bÃ i toÃ¡n há»“i quy

á»’, **ngoÃ i ra mÃ¬nh tháº¥y hÃ m nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh khoáº£ng cÃ¡ch Ä‘á»u cÃ³ kháº£ nÄƒng lÃ  má»™t hÃ m loss, há»¯u dá»¥ng khÃ´ng thÃ¬ chÆ°a cÃ³ cháº¯c**
### 2.3.1 Mean Squared Error
Hay cÃ²n Ä‘Æ°á»£c biáº¿t Ä‘áº¿n lÃ  L2 Loss, lÃ  má»™t hÃ m loss ráº¥t hay gáº·p, Ä‘Æ¡n giáº£n, dá»… dÃ¹ng, hiá»‡u quáº£, song váº«n cÃ³ nhÆ°á»£c Ä‘iá»ƒm (Ä‘á» cáº­p sau), dÆ°á»›i Ä‘Ã¢y lÃ  cÃ´ng thá»©c: 

$$
\begin{equation}
L(y, y') = \frac{1}{n} \sum_{i=1}^{n} (y_i - y'_i)^2
\end{equation}
$$

Trong Ä‘Ã³:

- $n$ lÃ  sá»‘ lÆ°á»£ng sample trong bá»™ dá»¯ liá»‡u (hoáº·c batch dá»¯ liá»‡u)
- $y_i$ lÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a máº«u thá»© $i$
- $y'$ lÃ  giÃ¡ trá»‹ thá»±c cá»§a máº«u thá»© $i$

HÃ m loss nÃ y sáº½ hoáº¡t Ä‘á»™ng vÃ´ cÃ¹ng tá»‘t náº¿u nhÆ° phÃ¢n phá»‘i cá»§a dá»¯ liá»‡u tuÃ¢n theo phÃ¢n phá»‘i chuáº©n, vÃ  khÃ´ng cÃ³ outlier. 
### 2.3.2 Huber Loss 

## 2.4 CÃ¡c hÃ m loss khÃ¡c

### 2.4.1 Kullback-Leibler divergence Loss (KL Loss)

### 2.4.2 Earth Mover's Distance (EMD)