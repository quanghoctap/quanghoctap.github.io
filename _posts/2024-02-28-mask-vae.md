---
title: 'Paper Explained 3: Masked Autoencoders Are Scalable Vision Learners'
date: 2024-02-28
categories: [Data Science, Deep Learning]
tags: [transformer, paper explained]
toc: true
math: true
publish: true
---

"Mask" aka ğŸ‘º lÃ  má»™t kÄ© thuáº­t Ä‘Æ°á»£c Æ°a chuá»™ng trong lÄ©nh vá»±c NLP. NÃ³ hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch che Ä‘i má»™t pháº§n trong dá»¯ liá»‡u, rá»“i Ä‘oÃ¡n cÃ¡i pháº§n bá»‹ che Ä‘Ã³ dá»±a vÃ o nhá»¯ng cÃ¡i khÃ´ng bá»‹ che ğŸ¤¨ğŸ‘Œ. Má»¥c tiÃªu cá»§a nÃ³ Ä‘Æ¡n giáº£n lÃ  Ä‘á»ƒ mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c biá»ƒu diá»…n chung trong dá»¯ liá»‡u, báº¥t ká»ƒ ngá»¯ cáº£nh cá»¥ thá»ƒ. Váº­y cÃ²n Ä‘á»‘i vá»›i dá»¯ liá»‡u ğŸ–¼ï¸ thÃ¬ cÃ¡i nÃ y xÃ i sao, giá» mÃ¬nh tÃ¬m hiá»ƒu trong bÃ i [nÃ y](https://arxiv.org/pdf/2111.06377.pdf) nháº¿ !!! ğŸ’ª.

# Giá»›i thiá»‡u 
NgÃ y nay máº¥y mÃ´ hÃ¬nh Deep Learning Ä‘Ã£ trá»Ÿ nÃªn quÃ¡ lÃ  máº¡nh cÅ©ng nhÆ° lÃ  cÃ¡i kháº£ nÄƒng cá»§a nÃ³ lÃ  ráº¥t lá»›n, káº¿t há»£p vá»›i viá»‡c pháº§n cá»©ng giá» máº¡nh hÆ¡n vÃ  chuyÃªn dá»¥ng hÆ¡n cho cÃ¡c bÃ i toÃ¡n liÃªn quan Ä‘áº¿n AI lÃ m cho viá»‡c con ngÆ°á»i train mÃ´ hÃ¬nh nhÆ° lÃ  diá»u gáº·p giÃ³ ğŸªğŸƒ. 

BÃªn lá» xÃ­u thÃ¬ Ä‘Ã¢y lÃ  dá»± Ä‘oÃ¡n cho thá»‹ trÆ°á»ng pháº§n cá»©ng cho AI tá»« nÄƒm 2022 cho tá»›i nÄƒm 2030.

![hardware](https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg)

ÄÃ³, nÃ³i chung lÃ  ráº¥t khá»§ng. NhÆ°ng mÃ  ae thá»­ nghá»‰, giá» cÃ¡i gÃ¬ cÅ©ng to ra, kiá»ƒu nhÆ° ae Ä‘ang tuá»•i Äƒn tuá»•i lá»›n Ä‘i, kháº©u pháº§n Äƒn cá»§a ae pháº£i nhiá»u hÆ¡n Ä‘Ãºng kh ? ğŸ¤” (rÃµ rÃ ng luÃ´n). ThÃ¬ máº¥y cÃ¡i model nÃ y cÅ©ng v, giá» mÃ´ hÃ¬nh lá»›n hÆ¡n cáº§n nhiá»u data hÆ¡n, kiá»ƒu kiá»ƒu váº­y Ä‘Ã³. TrÆ°á»›c Ä‘Ã³ train 1M áº£nh cÃ²n khÃ³ khÄƒn chá»© giá» ae train cáº£ chá»¥c M áº£nh trong phÃ²ng cÅ©ng daijoubu thoi. NhÆ°ng mÃ  váº¥n Ä‘á» nÃ¨ ae: **ğŸ¥¹ ? ÄÃ o Ä‘Ã¢u ra data giá» nÃ­ ? ğŸ¥¹**

KhÃ³ tráº£ lá»i liá»n Ä‘Ãºng kh ae =)))))))))))). Äá»ƒ cho ae cÃ³ cÃ¡i nhÃ¬n cá»¥ thá»ƒ hÆ¡n vá» cÃ¡i thá»‹ trÆ°á»ng cÅ©ng gá»i lÃ  ğŸ¤‘ğŸ«°ğŸ’µğŸ’¶ğŸ’· thÃ¬ dÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡i hÃ¬nh (nÃ³i chung tá»« nÄƒm 2019 láº­n) dá»± Ä‘oÃ¡n tÄƒng trÆ°á»Ÿng tá»›i 2025.

![labeling_cost](https://assets-global.website-files.com/62cd5ce03261cb3e98188470/62cd5ce03261cb756e1885e6_1*8aZc2rNfDVjvZ2Qe1Af52g.png)

ÄÃ³, nÃ³i chung lÃ  hao tiá»n. NhÆ°ng mÃ  cÃ³ cÃ¡ch nÃ o khÃ¡c Ä‘á»ƒ counter khÃ´ng? CÃ¢u tráº£ lá»i lÃ  YES. CÃ¡i sá»± hÃ¡u Äƒn cá»§a cÃ¡c model ngÃ y nay cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng phÆ°Æ¡ng phÃ¡p âœ¨self-supervised learningâœ¨. 

Cá»¥ thá»ƒ hÆ¡n thÃ¬ cÃ³ má»™t cÃ¡i task cá»¥ thá»ƒ trong NLP, Ä‘Ã³ lÃ  Masking Task, thÃ¬ cÃ¡i nÃ y Ä‘Æ¡n giáº£n lÃ  nÃ³ huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng cÃ¡ch dá»± Ä‘oÃ¡n cÃ¡c tá»« bá»‹ thiáº¿u trong cÃ¢u ğŸ‘ºğŸ”®. Cá»¥ thá»ƒ hÆ¡n thÃ¬ Ä‘áº§u tiÃªn nÃ³ tiáº¿n hÃ nh che vÃ i tá»« trong cÃ¢u (Ä‘Æ°Æ¡ng nhiÃªn lÃ  Ã­t tá»« thÃ´i nhÃ©, chá»© che nhiá»u quÃ¡ thÃ¬ khÃ´ng dá»± Ä‘oÃ¡n Ä‘Æ°á»£cğŸ‘½ğŸ‘Œ) vÃ  sá»­ dá»¥ng cÃ¡c tá»« cÃ²n láº¡i Ä‘á»ƒ Ä‘oÃ¡n ra tá»« Ä‘Ã³. NhÆ° cÃ¡i hÃ¬nh á»Ÿ dÆ°á»›i nÃ y lÃ  má»™t cÃ¡i vÃ­ dá»¥ cá»¥ thá»ƒ:

![MLM](/assets/img/blog3/mlm.png) 

ThÃ¬ cÃ¡i lá»£i Ã­ch cá»§a cÃ¡i viá»‡c nÃ y lÃ  nÃ³ cho phÃ©p mÃ´ hÃ¬nh há»c cÃ¡c tá»« dá»±a trÃªn ngá»¯ cáº£nh (Ä‘Æ°Æ¡ng nhiÃªn lÃ  khÃ´ng cáº§n label ğŸ¥‚). CÃ¢u há»i lÃ  Ä‘á»‘i vá»›i hÃ¬nh áº£nh, mÃ¬nh lÃ m váº­y Ä‘Æ°á»£c khÃ´ng? NÃ³i luÃ´n lÃ  Ä‘Æ°á»£c nhÃ©! Giá» mÃ¬nh lÃ m cÃ¡i nÃ y nÃ¨ ae, paper gá»‘c má»i ngÆ°á»i cÃ³ thá»ƒ Ä‘á»c á»Ÿ [Ä‘Ã¢y](https://arxiv.org/pdf/2111.06377.pdf) nha! 

Trong cÃ¡i bÃ i nghiÃªn cá»©u nÃ y, cÃ¡c tÃ¡c giáº£ Ä‘áº·t ra cÃ¢u há»i: **Äiá»u gÃ¬ lÃ m cho masked autoencoder khÃ¡c biá»‡t giá»¯a dá»¯ liá»‡u dáº¡ng chá»¯ vÃ  dá»¯ liá»‡u dáº¡ng áº£nh?**. VÃ  há» tráº£ lá»i cÃ¢u há»i nÃ y báº±ng viá»‡c Ä‘áº·t ra 3 váº¥n Ä‘á» chÃ­nh:

- **KhÃ¡c biá»‡t trong cáº¥u trÃºc mÃ´ hÃ¬nhğŸ—ï¸**: ThÃ´ng thÆ°á»ng, khi sá»­ dá»¥ng dá»¯ liá»‡u cho dáº¡ng áº£nh thÃ¬ ngÆ°á»i ta dÃ¹ng máº¥y cáº¥u trÃºc CNN, cÃ²n cho dá»¯ liá»‡u vÄƒn báº£n thÃ¬ ngÆ°á»i ta dÃ¹ng RNN hoáº·c lÃ  Transformer. CÃ¡i váº¥n Ä‘á» máº¥u chá»‘t khiáº¿n cho ae khÃ´ng Ã¡p dá»¥ng cÃ¡i masking thÃ´ng thÆ°á»ng cho máº¥y máº¡ng CNN lÃ  bá»Ÿi vÃ¬ cÃ¡i phÃ©p tÃ­ch cháº­p nÃ³ chá»‰ hoáº¡t Ä‘á»™ng ok vá»›i nhá»¯ng cÃ¡i grid thÃ´ng thÆ°á»ng chá»© nÃ³ khÃ´ng cÃ³ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c cÃ¡i nÃ o bá»‹ mask vÃ  pháº£i deal sao vá»›i cÃ¡i cell bá»‹ mask trong cÃ¡i grid Ä‘Ã³. ae tháº¥y váº¥n Ä‘á» chÆ°a? NhÆ°ng mÃ  thanks to cáº¥u trÃºc Transformer, cá»¥ thá»ƒ lÃ  mÃ´ hÃ¬nh ViT, mÃ  cÃ¡i nÃ y khÃ´ng cÃ²n lÃ  chÆ°á»›ng ngáº¡i ná»¯a. NÃªn táº¡m thá»i má»™t váº¥n Ä‘á» Ä‘Æ°á»£c giáº£i quyáº¿t.

- **KhÃ¡c biá»‡t trong máº­t Ä‘á»™ thÃ´ng tinğŸ¤¨**: ThÃ¬ cÃ¡i máº­t Ä‘á»™ thÃ´ng tin trong tá»«ng loáº¡i dá»¯ liá»‡u lÃ  khÃ¡c nhau. ae tÆ°á»Ÿng tÆ°á»£ng ngÃ´n ngá»¯ lÃ  do con ngÆ°á»i quy Ä‘á»‹nh, do Ä‘Ã³ trong vÄƒn báº£n, nÃ³ mang tÃ­nh ngá»¯ nghÄ©a cao cÅ©ng nhÆ° máº­t Ä‘á»™ thÃ´ng tin ráº¥t dÃ y Ä‘áº·c, háº§u háº¿t má»i cÃ¢u ae thá»‘t ra Ä‘á»u dÃ­nh dÃ­nh vá»›i nhau, chá»© khÃ´ng pháº£i kiá»ƒu cÃ¢u dÃ i 4 chá»¯ mÃ  4 chá»¯ cháº£ liÃªn quan gÃ¬ nhau =))))))))))) Ä‘á»c nÃ³ cá»© cáº¥n cáº¥n. HÃ¬nh áº£nh thÃ¬ ngÆ°á»£c láº¡i, hÃ¬nh áº£nh tá»“n táº¡i trong tá»± nhiÃªn, con ngÆ°á»i khÃ´ng cÃ³ 'quy ra hÃ¬nh áº£nh' Ä‘Æ°á»£c, ngoÃ i ra hÃ¬nh áº£nh cÃ²n lÃ  loáº¡i dá»¯ liá»‡u cÃ³ tÆ°Æ¡ng quan khÃ´ng gian cao (nÃ y cá»¥m gá»‘c lÃ  heavy spatial redundancy vÃ  Ä‘á» cáº­p Ä‘áº¿n má»‘i liÃªn há»‡ máº­t thiáº¿t giá»¯a cÃ¡c Ä‘iá»ƒm áº£nh trong khÃ´ng gian). Do Ä‘Ã³ mÃ  Ä‘á»‘i vá»›i hÃ¬nh áº£nh, bá»‹ há»¥t má»™t vÃ i chá»— thÃ´i thÃ¬ váº«n dÃ¹ng cÃ¡c dá»¯ liá»‡u lÃ¢n cáº­n Ä‘á»ƒ tÃ¡i cáº¥u trÃºc láº¡i pháº§n bá»‹ khuyáº¿t Ä‘Ã³ chá»‰ cáº§n mÃ´ hÃ¬nh hiá»ƒu Ä‘Æ°á»£c má»™t vÃ i báº£n cháº¥t cÆ¡ báº£n cá»§a hÃ¬nh áº£nh nhÆ° lÃ  Ä‘á»‘i tÆ°á»£ng trong áº£nh, khung cáº£nh trong áº£nh, v.v... MÃ  nhÆ° váº­y thÃ¬ náº¿u nhÆ° chá»‰ che má»™t xÃ­u nhÆ° bÃªn dá»¯ liá»‡u vÄƒn báº£n thÃ´i thÃ¬ dá»… quÃ¡ ğŸ¤¨ğŸ‘ŒğŸ‘. NÃªn nhÃ³m tÃ¡c giáº£ cÃ³ Ä‘á» xuáº¥t mÃ¬nh sáº½ mask pháº§n lá»›n áº£nh, cÃ³ khi mask tá»›i 85% áº£nh luÃ´n, khÃ´ng váº¥n Ä‘á» gÃ¬ cáº¡! ğŸ’¯. CÃ³ 2 má»¥c Ä‘Ã­ch chÃ­nh: **Giáº£m thiá»ƒu dÆ° thá»«a trong tÃ­nh toÃ¡n** (chá»© che xÃ­u thÃ¬ dá»… quÃ¡, tÃ­nh chi ná»¯a, vá»›i cáº£ lÃ m váº­y thÃ¬ pháº£i tÃ­nh toÃ¡n ráº¥t nhiá»u mÃ  hiá»‡u quáº£ mang láº¡i ráº¥t Ã­t) vÃ  **Táº¡o ra má»™t task Ä‘á»§ khÃ³ Ä‘á»ƒ hiá»ƒu nhiá»u hÆ¡n vá» táº¥m áº£nh chá»© khÃ´ng chá»‰ Ä‘Æ¡n giáº£n lÃ  vÃ i báº£n cháº¥t cÆ¡ báº£n cá»§a táº¥m áº£nh** (Ä‘iá»u nÃ y Ã©p cho mÃ´ hÃ¬nh pháº£i há»c nhá»¯ng cÃ¡i khÃ³ hÆ¡n, tá»« Ä‘Ã³ lÃ m tá»‘t hÆ¡n). 

- **Pháº§n decoder**: CÃ³ má»™t sá»± Ä‘á»‘i láº­p trong pháº§n decoder khi dÃ¹ng Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡i mask cho dá»¯ liá»‡u áº£nh vÃ  dá»¯ liá»‡u vÄƒn báº£n. Hiá»ƒu Ä‘Æ¡n giáº£n thÃ¬ Ä‘á»‘i vá»›i dá»¯ liá»‡u áº£nh, pháº§n nÃ y sáº½ tÃ¡i cáº¥u trÃºc láº¡i á»Ÿ pixel-level, nhÆ° váº­y cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¡c cÃ¡i pixel Ä‘Æ°á»£c tÃ¡i cáº¥u trÃºc nÃ y cÃ³ tÃ­nh ngá»¯ nghÄ©a tháº¥p. Trong khi Ä‘Ã³ Ä‘á»‘i vá»›i dá»¯ liá»‡u vÄƒn báº£n thÃ¬ khÃ¡c, nhÆ° nÃ£y mÃ¬nh nÃ³i lÃ  vÄƒn báº£n lÃ  loáº¡i dá»¯ liá»‡u cÃ³ tÃ­nh ngá»¯ nghÄ©a dÃ y Ä‘áº·c, thÃ¬ cÃ¡i nÃ y tá»« Ä‘Æ°á»£c dá»¯ Ä‘oÃ¡n cÅ©ng pháº£i cÃ³ tÃ­nh ngá»¯ nghÄ©a cao. 

VÃ  tá»« nhá»¯ng nháº­n xÃ©t trÃªn mÃ  nhÃ³m tÃ¡c giáº£ Ä‘Ã£ Ä‘á» xuáº¥t ra mÃ´ hÃ¬nh MAE (Masked AutoEncoder) Ä‘á»ƒ há»c cÃ¡c Ä‘áº·t trÆ°ng cá»§a áº£nh. Vá»›i mÃ´ táº£ ngáº¯n gá»n (chi tiáº¿t vÃ´ sau) vá» mÃ´ hÃ¬nh. MAE sáº½ che máº¥y cÃ¡i patches tá»« cÃ¡i áº£nh Ä‘áº§u vÃ o rá»“i tÃ¡i cáº¥u trÃºc nÃ³ á»Ÿ pixel-level. VÃ  khÃ´ng nhÆ° nhá»¯ng cÃ¡i AE mÃ  ae hay gáº·p, cÃ¡i AE mÃ  tÃ¡c giáº£ giá»›i thiá»‡u lÃ  má»™t cáº¥u trÃºc báº¥t Ä‘á»‘i xá»©ng (báº¥t Ä‘á»‘i xá»©ng trong AE cÃ³ nghÄ©a lÃ  pháº§n encoder vÃ  decoder cÃ³ cÃ¡i kÃ­ch thÆ°á»›c khÃ¡c nhau). Trong khi pháº§n encoder chá»‰ hoáº¡t Ä‘á»™ng Ä‘á»‘i vá»›i nhá»¯ng pháº§n khÃ´ng bá»‹ che thÃ¬ pháº§n decoder hoáº¡t Ä‘á»™ng luÃ´n cáº£ pháº§n bá»‹ che láº«n khÃ´gn bá»‹ che. NÃ³i tÃ³m gá»n thÃ¬ lÃ  nhÆ° váº­y, mÃ¬nh sáº½ giáº£i thÃ­ch cá»¥ thá»ƒ hÆ¡n á»Ÿ pháº§n sau, trÆ°á»›c máº¯t má»i ngÆ°á»i cÃ³ thá»ƒ xem cÃ¡i hÃ¬nh nÃ y Ä‘á»ƒ náº¯m Ä‘Æ°á»£c cÃ¡i mÃ´ hÃ¬nh nÃ y trÃ´ng ra sao: 

![MAE](/assets/img/blog3/MAE.png)

VÃ  khi mÃ¬nh nÃ³i tá»›i viá»‡c tÃ¡i cáº¥u trÃºc láº¡i hÃ¬nh thÃ¬ lÃ  nhÆ° sau:

![Reconstructed](/assets/img/blog3/reconstruct.png)

ÄÃ³, trÆ°á»›c máº¯t thÃ¬ nÃ³ Ä‘Æ¡n giáº£n váº­y thÃ´i Ã , Ä‘á»ƒ cá»¥ thá»ƒ hÆ¡n thÃ¬ mÃ¬nh qua pháº§n mÃ´ hÃ¬nh sau hen !ğŸ‘½

**REFERENCES**

(1) [https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg](https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg)

(2) [https://www.lightly.ai/post/ai-human-bottleneck](https://www.lightly.ai/post/ai-human-bottleneck)

(3) [https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5](https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5)

(4) [https://arxiv.org/pdf/2111.06377](https://arxiv.org/pdf/2111.06377)

# CÃ¡c khÃ¡i niá»‡m liÃªn quan

## Masked Language Modeling (MÃ´ hÃ¬nh ngÃ´n ngá»¯ há»c tá»« ngá»¯ cáº£nh)
CÃ¡i MLM lÃ  má»™t trong cÃ¡c kÄ© thuáº­t NLP ráº¥t thÃ nh cÃ´ng trong viá»‡c pre-trained máº¥y cÃ¡i mÃ´ hÃ¬nh deep learning. CÃ¡i mÃ´ hÃ¬nh BERT mÃ  cÃ³ thá»ƒ máº¥y báº¡n quen thuá»™c cÅ©ng sá»­ dá»¥ng kÄ© thuáº­t nÃ y Ä‘á»ƒ tiáº¿n hÃ nh pre-trained. VÃ  cÅ©ng cÃ³ nhá»¯ng bÃ i nghiÃªn cá»©u Ä‘Ã£ cho tháº¥y ráº±ng cÃ¡i phÆ°Æ¡ng phÃ¡p nÃ y lÃ  siÃªu hiá»‡u quáº£ khi mÃ  mÃ¬nh scale nÃ³ lÃªn nhÆ° bÃ i [nÃ y](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html). BÃªn cáº¡nh Ä‘Ã³ khi thá»±c nghiá»‡m thÃ¬ cÅ©ng cÃ³ ráº¥t nhiá»u á»©ng dá»¥ng khi Ã¡p dá»¥ng pretrained model theo phÆ°Æ¡ng phÃ¡p nÃ y cho ra cÃ¡c káº¿t quáº£ ráº¥t tá»‘t. 

ThÃ¬ cÃ¡i nÃ y nhÆ° mÃ¬nh Ä‘á» cáº­p á»Ÿ trÃªn, lÃ  mÃ¬nh sáº½ che bá»›t vÃ i tá»« trong cÃ¢u vÃ  dÃ¹ng nhá»¯ng tá»« cÃ²n láº¡i Ä‘á»ƒ dá»± Ä‘oÃ¡n nhá»¯ng tá»« Ä‘Ã³, vÃ  táº¥t nhiÃªn xÃ©t vá» máº·t ngá»¯ nghÄ©a thÃ¬ cÃ¡c tá»« nÃ y ráº¥t cÃ³ Ã½ nghÄ©a bá»Ÿi vÃ¬ tÃ­nh cháº¥t cá»§a cÃ¢u mÃ  mÃ¬nh Ä‘á» cáº­p lÃºc nÃ£y.

Ok, váº­y lÃ  giá» chÃºng ta Ä‘Ã£ hiá»ƒu cÃ¡i MLM lÃ  nÃ³ lÃ m gÃ¬ rá»“i Ä‘Ãºng kh, nhÆ°ng mÃ  cá»¥ thá»ƒ nÃ³ lÃ m ra sao thÃ¬ má»i ngÆ°á»i cÃ³ thá»ƒ Ä‘á»c thÃªm á»Ÿ bÃ i [nÃ y](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t nháº¿. Náº¿u Ä‘Æ°á»£c thÃ¬ cháº¯c mÃ¬nh sáº½ lÃªn má»™t bÃ i nÃ³i vá» BERT vÃ  náº¿u Ä‘Æ°á»£c thi cháº¯c code láº¡i cÃ¡i báº£n lightweight cá»§a nÃ³ cÅ©ng Ä‘Æ°á»£c.

## Autoencoder (Bá»™ tá»± mÃ£ hÃ³a)
CÃ¡i autoencoder nÃ y lÃ  má»™t phÆ°Æ¡ng phÃ¡p cá»• Ä‘iá»ƒn Ä‘á»ƒ há»c cÃ¡c biá»ƒu diá»…n cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o. Háº§u nhÆ° bá»™ autoencoder nÃ o cÅ©ng sáº½ gá»“m 2 pháº§n chÃ­nh (mÃ¬nh chÆ°a tháº¥y cÃ¡i nÃ o cÃ³ hÆ¡n 2 bá»™ ğŸ˜) Ä‘Ã³ lÃ  pháº§n encoder giÃºp Ã¡nh xáº¡ biá»ƒu diá»…n cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o sang má»™t cÃ¡i khÃ´ng gian (thÆ°á»ng lÃ  cÃ³ sá»‘ chiá»u nhá» hÆ¡n) gá»i lÃ  'latent space' vÃ  tá»« cÃ¡i khÃ´ng gian nÃ y, decoder sáº½ cá»‘ gáº¯ng Ã¡nh xáº¡ nÃ³ vá» khÃ´ng gian ban Ä‘áº§u (nÃ³i cá»¥ thá»ƒ hÆ¡n lÃ  cÃ¡i decoder sáº½ cá»‘ gáº¯ng tÃ¡i cáº¥u trÃºc láº¡i dá»¯ liá»‡u Ä‘áº§u vÃ o tá»« cÃ¡i vector Ä‘áº¡i diá»‡n dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘Ã³ trong cÃ¡i latent space). VÃ  bá»™ nÃ y thÃ´ng thÆ°á»ng sáº½ nhÃ¬n nhÆ° sau:

![Autoencoder](/assets/img/blog3/AE.png)

Má»™t pháº§n nhá» hÆ¡n cá»§a lá»›p autoencoder Ä‘Ã³ lÃ  DAE (Denoising autoencoder). Nhá»¯ng cÃ¡i mÃ´ hÃ¬nh cÃ³ cáº¥u trÃºc DAE thÆ°á»ng sáº½ nháº­n vÃ o má»™t tÃ­n hiá»‡u Ä‘áº§u vÃ o Ä‘Ã£ bá»‹ nhiá»…u (paper dÃ¹ng tá»« corrupt), sau Ä‘Ã³ cho vÃ o latent space, sau Ä‘Ã³ decoder sáº½ cá»‘ gáº¯ng tÃ¡i cáº¥u trÃºc láº¡i tÃ­n hiá»‡u Ä‘áº§u vÃ o khÃ´ng bá»‹ nhiá»…u. VÃ  nÃ³ cÅ©ng cÃ³ nhiá»u hÆ°á»›ng tiáº¿p cáº­n sá»­ dá»¥ng cÃ¡i DAE nÃ y láº¯m mng, kiá»ƒu nhÆ° Ä‘á»‘i vá»›i dá»¯ liá»‡u dáº¡ng áº£nh, ngÆ°á»i ta cÃ³ thá»ƒ rÃºt bá»›t 1 channel, hoáº·c lÃ  xÃ³a Ä‘i vÃ i pixel trong áº£nh, v.v.... NÃ³i chung cÃ¡i mÃ´ hÃ¬nh MAE mÃ  hÃ´m nay chÃºng mÃ¬nh tÃ¬m hiá»ƒu cÃ³ thá»ƒ xem nhÆ° lÃ  má»™t pháº§n cá»§a cÃ¡i cáº¥u trÃºc DAE. 

## Masked Image Encoding 
CÃ¡i nÃ y nghe thÃ¬ cÃ³ váº» hao hao cÃ¡i MLM á»Ÿ trÃªn Ä‘Ã³, nhÆ°ng Ä‘á»ƒ cá»¥ thá»ƒ hÆ¡n, thÃ¬ cÃ¡i phÆ°Æ¡ng phÃ¡p nÃ y sáº½ há»c Ä‘Æ°á»£c nhá»¯ng biá»ƒu diá»…n tá»« cÃ¡i áº£nh bá»‹ há»ng (corrupt) do masking. Hoáº·c lÃ  thÃªm nhiá»…u vÃ o áº£nh, hoáº·c lÃ  inpaint áº£nh rá»“i dÃ¹ng CNN Ä‘á»ƒ khÃ´i phá»¥c, v.v... NÃ³i chung lÃ  nhiá»u mÃ  mÃ¬nh cÅ©ng chÆ°a tÃ¬m hiá»ƒu háº¿t Ä‘á»‘ng nÃ y.

## Self-supervised learning (Há»c tá»± giÃ¡m sÃ¡t)
Gá»i lÃ  tá»± giÃ¡m sÃ¡t bá»Ÿi vÃ¬ cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y sáº½ tá»± táº¡o ra cÃ¡c "giÃ¡m sÃ¡t" riÃªng cá»§a mÃ¬nh báº±ng cÃ¡ch tÃ¬m kiáº¿m cÃ¡c cáº¥u trÃºc vÃ  má»‘i quan há»‡ áº©n trong dá»¯ liá»‡u Ä‘á»ƒ há»c Ä‘Æ°á»£c cÃ¡c biá»ƒu diá»…n cÃ³ Ã­ch cá»§a dá»¯ liá»‡u. VÃ  cÃ¡i nhÆ°á»£c Ä‘iá»ƒm cá»§a nÃ³ nhÆ° Ä‘áº§u bÃ i viáº¿t mÃ¬nh cÃ³ Ä‘á» cáº­p Ä‘Ã³ lÃ  khÃ´ng tá»‘n tiá»n thuÃª nhÃ¢n cÃ´ng gáº¯n nhÃ£n thá»§ cÃ´ng. NhÆ°ng mÃ  rÃµ rÃ ng lÃ  náº¿u lÃ m váº­y thÃ¬ performance cá»§a nÃ³ khÃ´ng cÃ³ Ä‘Ã¡ vÃ´ Ä‘áº§u cÃ¡i tháº±ng supervised learning Ä‘Æ°á»£c. NÃªn lÃ  pháº£i Ä‘Ã¡nh Ä‘á»•i thÃ´i. 

Tuy nhiÃªn cÃ¡ch tiáº¿p cáº­n nÃ y ráº¥t lÃ  há»¯u dá»¥ng náº¿u ae muá»‘n giáº£i quyáº¿t má»™t cÃ¡i downstream task nÃ o Ä‘Ã³. Bá»Ÿi vÃ¬ sau khi Ä‘Ã£ qua nhiá»u cÃ¡i pretext task nhÆ° next sentence prediction hay lÃ  MLM hay lÃ  tá»± fill kÃªnh mÃ u hay lÃ  gÃ¬ gÃ¬ Ä‘áº¥y thÃ¬ model Ä‘Ã£ há»c Ä‘Æ°á»£c biá»ƒu diá»…n cá»§a dá»¯ liá»‡u ráº¥t tá»‘t rá»“i. 

**REFERENCES**

(1) [https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)

(2) [https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c)

# Tá»•ng quan mÃ´ hÃ¬nh
MÃ´ hÃ¬nh MAE mÃ  tÃ¡c giáº£ Ä‘á» cáº­p lÃ  má»™t biáº¿n thá»ƒ hay nÃ³i Ä‘Ãºng hÆ¡n lÃ  thuá»™c cáº¥u trÃºc DAE (denoising autoencoder) mÃ¬nh cÃ³ Ä‘á» cáº­p lÃºc nÃ£y. VÃ  cÅ©ng nhÆ° bao nhiÃªu cáº¥u trÃºc AE khÃ¡c thÃ´i, pháº§n encoder sáº½ Ã¡nh xáº¡ tÃ­n hiá»‡u Ä‘áº§u vÃ o (trong trÆ°á»ng há»£p nÃ y lÃ  má»™t biá»ƒu diá»…n thÆ°a cá»§a hÃ¬nh áº£nh do chÃºng ta Ä‘Ã£ thá»±c hiá»‡n masking rá»“i), mÃ  cá»¥ thá»ƒ hÆ¡n thÃ¬ trong MAE, pháº§n encoder sáº½ chá»‰ tÃ­nh toÃ¡n dá»±a trÃªn cÃ¡c pháº§n khÃ´ng bá»‹ mask thÃ´i (tá»©c lÃ  cÃ¡c pháº§n mÃ  cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c). VÃ  rá»“i pháº§n decoder sáº½ cá»‘ gáº¯ng tÃ¡i cáº¥u trÃºc tÃ­n hiá»‡u ban Ä‘áº§u (biá»ƒu diá»…n hÃ¬nh áº£nh hoÃ n chá»‰nh lÃºc chÆ°a bá»‹ masking) tá»« cÃ¡i latent space Ä‘Ã³. 

## Masking
Trong paper nÃ y, masking cá»§a cÃ¡c patch áº£nh (theo ngÃ´n ngá»¯ cá»§a ViT) Ä‘Æ°á»£c set á»Ÿ má»™t tá»· lá»‡ ráº¥t nhanh, trong paper nÃ³ láº¥y lÃ  85%, nÃ³i nÃ´m na, 85% cá»§a táº¥m áº£nh sáº½ bá»‹ che Ä‘i, nhiá»‡m vá»¥ cá»§a 15% pixel cÃ²n láº¡i trong áº£nh lÃ  cá»‘ gáº¯ng tÃ¡i cáº¥u trÃºc láº¡i 85% Ä‘Ã³. Nghe thÃ¬ nÃ³ buá»“n cÆ°á»i, nhÆ°ng mÃ  nÃ³ láº¡i work well =))))))))))))))))). 

## MAE encoder

## MAE decoder

## Objective function vÃ  Optimizer


# á»¨ng dá»¥ng mÃ´ hÃ¬nh

# Tháº£o luáº­n thÃªm vÃ  nháº­n xÃ©t
Trong lÃºc viáº¿t bÃ i nÃ y thÃ¬ mÃ¬nh suy nghÄ© vu vÆ¡, giá» láº¥y vÃ­ dá»¥ nhÆ° má»i ngÆ°á»i Ä‘ang cÃ³ má»™t cÃ¡i prompt vá»›i má»™t cÃ¡i áº£nh vÃ  cÃ¡i prompt Ä‘ang táº£ cÃ¡i áº£nh Ä‘Ã³ Ä‘i. CÃ³ cÃ¡i task nÃ o Ä‘Ã³ kiá»ƒu nhÆ° má»i ngÆ°á»i Ä‘ang cÃ³ cÃ¡i hÃ¬nh con nguá»“i Ä‘á»©ng cáº¡nh con chÃ³, vÃ  cÃ³ cÃ¡i prompt ghi i xÃ¬ váº­y luÃ´n, rá»“i má»i ngÆ°á»i mask con chÃ³ trong táº¥m hÃ¬nh Ä‘i, rá»“i má»i ngÆ°á»i tá»« cÃ¡i prompt, cá»‘ gáº¯ng reconstruct láº¡i con chÃ³ Ä‘Ã³. Hoáº·c lÃ  má»i ngÆ°á»i cÃ³ thá»ƒ lÃ m cÃ¡i NER task Ä‘á»ƒ enhance cÃ¡i representation cá»§a cÃ¢u vá»›i áº£nh ğŸ˜ğŸ˜ğŸ˜might work tho