---
title: 'Paper Explained 6: High Resolution Image Synthesis with Latent Diffusion Models'
date: 2027-04-08
categories: [Data Science, Deep Learning]
tags: [advanced, paper explained]     # TAG names should always be lowercase
toc: true
math: true
publish: true
---

Stable Diffusion lÃ  má»™t mÃ´ hÃ¬nh trÃ­ tuá»‡ nhÃ¢n táº¡o (AI) táº¡o sinh Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Stability AI. NÃ³ cho phÃ©p báº¡n táº¡o ra hÃ¬nh áº£nh tá»« mÃ´ táº£ báº±ng vÄƒn báº£n. Stable Diffusion hoáº¡t Ä‘á»™ng dá»±a trÃªn mÃ´ hÃ¬nh khuáº¿ch tÃ¡n, báº¯t Ä‘áº§u vá»›i má»™t bá»©c tranh nhiá»…u vÃ  dáº§n dáº§n "khuáº¿ch tÃ¡n" nhiá»…u Ä‘Ã³ Ä‘á»ƒ táº¡o ra hÃ¬nh áº£nh mong muá»‘n. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c Ä‘iá»u khiá»ƒn bá»Ÿi mÃ´ táº£ báº±ng vÄƒn báº£n mÃ  báº¡n cung cáº¥p. Stable Diffusion cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng cho nhiá»u má»¥c Ä‘Ã­ch khÃ¡c nhau, bao gá»“m táº¡o ra nghá»‡ thuáº­t, minh há»a Ã½ tÆ°á»Ÿng, táº¡o áº£nh cho cÃ¡c má»¥c Ä‘Ã­ch thÆ°Æ¡ng máº¡i vÃ  nhiá»u hÆ¡n ná»¯a. 

# Giá»›i thiá»‡u vá» Stable Diffusion
TrÆ°á»›c tiÃªn Ä‘á»ƒ báº¯t Ä‘áº§u cho viá»‡c tÃ¬m hiá»ƒu vá» Stable Diffusion, thÃ¬ mÃ¬nh sáº½ giá»›i thiá»‡u vá» mÃ´ hÃ¬nh sinh trÆ°á»›c. Sau Ä‘Ã³ mÃ¬nh sáº½ giá»›i thiá»‡u vá» Ä‘á»™ng lá»±c Ä‘á»ƒ cÃ¡c tÃ¡c giáº£ táº¡o ra mÃ´ hÃ¬nh nÃ y (fun fact â˜ï¸ğŸ¤“: stable diffusion lÃ  báº£n cáº£i tiáº¿n tá»« mÃ´ hÃ¬nh diffusion cÃ³ tá»« 2015) vÃ  sau Ä‘Ã³ lÃ  cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a mÃ´ hÃ¬nh nÃ y, cuá»‘i cÃ¹ng sáº½ lÃ  pháº§n code Ä‘á»ƒ implement láº¡i tá»« Ä‘áº§u mÃ´ hÃ¬nh nÃ y. 

Má»™t vÃ i á»©ng dá»¥ng cá»§a Stable Diffusion (mÃ  cÃ³ thá»ƒ cÃ¡c báº¡n Ä‘Ã£ tháº¥y hoáº·c chÆ°a tháº¥y):
![example](/assets/img/paper%20explained%206/example.png)
_Example for 1.45B Model with user input_

VÃ  ngoÃ i ra thÃ¬ cÅ©ng cÃ³ nhiá»u phiÃªn báº£n cáº£i tiáº¿n khÃ¡c, xá»‹n hÆ¡n ná»¯a. 
# CÃ¡c kiáº¿n thá»©c liÃªn quan 
## 1. MÃ´ hÃ¬nh sinh (Generative model)

![genvsdis](/assets/img/paper%20explained%206/genvsdis.png)
_Discriminative versus Generative model_

Giá»›i thiá»‡u sÆ¡ qua thÃ¬ Discriminative model lÃ  mÃ´ hÃ¬nh phÃ¢n loáº¡i (hoáº·c lÃ  mÃ´ hÃ¬nh Ä‘iá»u kiá»‡n (conditional models) nhÆ° má»™t vÃ i nÆ¡i khÃ¡c Ä‘áº·t tÃªn), loáº¡i mÃ´ hÃ¬nh nÃ y sáº½ há»c cÃ¡c Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh (decision boundaries) Ä‘á»ƒ cho ra cÃ¡c Ä‘Ã¡p Ã¡n lÃ  cÃ³ hoáº·c khÃ´ng, Ä‘Ãºng hoáº·c sai, tháº¯ng hoáº·c thua, vÃ¢n vÃ¢n. NgoÃ i ra loáº¡i mÃ´ hÃ¬nh nÃ y cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¡t triá»ƒn lÃªn Ä‘á»ƒ phÃ¢n loáº¡i Ä‘a nhÃ£n chá»© khÃ´ng pháº£i nhÃ£n nhá»‹ phÃ¢n nhÆ° vá»«a vÃ­ dá»¥. NhÆ°ng do chá»©c nÄƒng cá»§a cÃ¡c Discriminative model lÃ  nhÆ° váº­y, nÃªn loáº¡i mÃ´ hÃ¬nh nÃ y **khÃ´ng cÃ³ kháº£ nÄƒng táº¡o ra cÃ¡c cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u má»›i**. 

MÃ´ hÃ¬nh sinh (Generative model) Ä‘Ãºng nhÆ° tÃªn gá»i cá»§a nÃ³ thÃ¬ láº¡i cho phÃ©p chÃºng ta **táº¡o ra cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u má»›i tuÃ¢n theo phÃ¢n phá»‘i xÃ¡c suáº¥t tÆ°Æ¡ng tá»± nhÆ° phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a dá»¯ liá»‡u mÃ  chÃºng ta huáº¥n luyá»‡n trÃªn**. Tá»« Ä‘Ã³ mÃ  má»Ÿ ra nhiá»u hÆ°á»›ng tiáº¿p cáº­n Ä‘a dáº¡ng (cho nhiá»u kiá»ƒu dá»¯ liá»‡u luÃ´n), vÃ­ dá»¥ nhÆ°:
- Sinh áº£nh (Image Synthesis)
- TÄƒng cÆ°á»ng dá»¯ liá»‡u (Data Augmentation)
- Viáº¿t nháº¡c, lÃ m thÆ¡, v.v...
 
Láº¥y má»™t vÃ­ dá»¥ thá»±c táº¿, báº¡n Ä‘ang muá»‘n táº¡o ra khoáº£ng 100 sample vá» chiá»u cao cá»§a ngÆ°á»i chÃ¢u Ã¡ Ä‘i, vÃ  qua cÃ¡c dá»¯ liá»‡u báº¡n cÃ³ Ä‘Æ°á»£c, báº¡n biáº¿t ráº±ng chiá»u cao cá»§a ngÆ°á»i ChÃ¢u Ã tuÃ¢n theo phÃ¢n phá»‘i chuáº©n $\mathcal{N}(160, 15^2)$ (sá»‘ mÃ¬nh xáº¡o Ä‘Ã³, nhÆ°ng mÃ  idea lÃ  váº­y ğŸ«¥). LÃºc nÃ y, cÃ¡c báº¡n sáº½ Ä‘áº·t tÃªn cá»§a biáº¿n ngáº«u nhiÃªn cá»§a cÃ¡c báº¡n lÃ  $X$, lÃºc nÃ y cÃ¡c báº¡n cÃ³ thá»ƒ biá»ƒu diá»…n dÆ°á»›i dáº¡ng toÃ¡n há»c nhÆ° sau: 

$$
\begin{equation}
X \sim \mathcal{N}(160, 15^2)
\end{equation}
$$

ThÃ¬ cÃ¡i mÃ´ hÃ¬nh sinh Ä‘Æ¡n giáº£n lÃ  váº­y, **má»™t mÃ´ hÃ¬nh sinh sáº½ há»c cÃ¡i phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a táº­p dá»¯ liá»‡u, sau Ä‘Ã³ ta cÃ³ thá»ƒ láº¥y máº«u tá»« cÃ¡i phÃ¢n phá»‘i Ä‘Æ°á»£c há»c Ä‘Ã³ Ä‘á»ƒ táº¡o ra dá»¯ liá»‡u má»›i**. 

Vá» cÆ¡ báº£n thÃ¬ nÃ³ sáº½ lÃ  váº­y, tuy nhiÃªn cÃ¡c báº¡n nÃªn tÃ¬m Ä‘á»c sÃ¢u hÆ¡n Ä‘á»ƒ hiá»ƒu tÆ°á»ng táº­n vá» váº¥n Ä‘á» nÃ y. 

## 2. MÃ´ hÃ¬nh khuáº¿ch tÃ¡n (Diffusion Model)
ThÆ°á»ng thÃ¬ ngÆ°á»i ta chá»‰ nÃ³i lÃ  Diffusion Model thÃ´i chá»© khÃ´ng ai dÃ¹ng tá»« mÃ´ hÃ¬nh khuáº¿ch tÃ¡n cáº£ (fun fact â˜ï¸ğŸ¤“: chá»¯ Diffusion trong nhiá»‡t Ä‘á»™ng há»c cÃ³ nghÄ©a lÃ  khuáº¿ch tÃ¡n (vÃ  Ä‘Ã¢y cÅ©ng lÃ  idea cá»§a bÃ i nÃ y luÃ´n)). 

![diffusion_schematics](/assets/img/paper%20explained%206/diffusion_schematics.png)
_Diffusion process_

HÃ¬nh á»Ÿ trÃªn lÃ  tÃ³m táº¯t toÃ n bá»™ Ã½ tÆ°á»Ÿng cá»§a Diffusion Model, Ä‘Ã³ lÃ  má»™t quÃ¡ trÃ¬nh bao gá»“m 2 bÆ°á»›c lá»›n:
- QuÃ¡ trÃ¬nh khuáº¿ch tÃ¡n thuáº­n (forward process): ThÃªm nhiá»…u má»™t cÃ¡ch **cháº­m rÃ£i, cÃ³ há»‡ thá»‘ng**.
- QuÃ¡ trÃ¬nh khuáº¿ch tÃ¡n nghá»‹ch (backward process): **Äáº£o ngÆ°á»£c** tá»« áº£nh nhiá»…u vá» láº¡i áº£nh ban Ä‘áº§u (nghe vÃ´ lÃ½ nhÆ°ng tháº­t ra cÃ³ thá»ƒ).

Vá» cá»¥ thá»ƒ 2 quÃ¡ trÃ¬nh nÃ y, cÃ¡c báº¡n Ä‘á»£i mÃ¬nh ra bÃ i má»›i nhÃ© =)))), hoáº·c cÃ¡c báº¡n cÃ³ thá»ƒ Ä‘á»c bÃ i [nÃ y](https://viblo.asia/p/diffusion-models-co-ban-phan-1-E1XVOx884Mz) (Viáº¿t báº±ng tiáº¿ng viá»‡t, ngáº¯n, gá»n, dá»… hiá»ƒu, tháº³ng vÃ o váº¥n Ä‘á»), hoáº·c Ä‘á»c bÃ i [nÃ y](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) (Viáº¿t báº±ng tiáº¿ng anh, dÃ i, kÄ©, ngÆ°á»i viáº¿t lÃ  lead cá»§a má»™t team trong OpenAI). 

## 3. Motivation 
Má»™t váº¥n Ä‘á» cÅ©ng náº±m á»Ÿ trong hÃ¬nh á»Ÿ trÃªn, thá»ƒ hiá»‡n Ä‘iá»ƒm yáº¿u cá»§a mÃ´ hÃ¬nh Diffusion truyá»n thá»‘ng, Ä‘Ã³ lÃ  á»Ÿ **kÃ­ch thÆ°á»›c cá»§a latent variable (biáº¿n tiá»m áº©n) báº±ng Ä‘Ãºng vá»›i kÃ­ch thÆ°á»›c cá»§a ma tráº­n áº£nh Ä‘áº§u vÃ o**, mÃ  má»™t khi chÃºng ta láº¥y cÃ¡i Ä‘Ã³, mang Ä‘i cho vÃ o mÃ´ hÃ¬nh khÃ¡c Ä‘á»ƒ denoise, láº¥y vÃ­ dá»¥ nhÆ° U-Net cháº³ng háº¡n, chÃºng ta sáº½ cÃ³ kÃ­ch thÆ°á»›c ma tráº­n ráº¥t lá»›n, dáº«n Ä‘áº¿n hao tá»•n vá» chi phÃ­ tÃ­nh toÃ¡n. Äá»‘i vá»›i cÃ¡c áº£nh Ä‘áº§u vÃ o quÃ¡ nhá», vÃ­ dá»¥ nhÆ° lÃ  MNIST vá»›i size áº£nh lÃ  `28 x 28` thÃ¬ khÃ´ng pháº£i váº¥n Ä‘á», nhÆ°ng má»i thá»© sáº½ bÃ©t nhÃ¨ náº¿u kÃ­ch thÆ°á»›c áº£nh lÃ  `1024 x 1024`. 

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Ä‘Ã³, nhÃ³m tÃ¡c giáº£ Ä‘Ã£ Ä‘á» xuáº¥t má»™t mÃ´ hÃ¬nh má»›i, mÃ  giá» chÃºng ta gá»i lÃ  Stable Diffusion (fun fact â˜ï¸ğŸ¤“: tÃªn gá»‘c cá»§a há» mÃ´ hÃ¬nh nÃ y lÃ  Latent Diffusion Model (LDM)). DÆ°á»›i Ä‘Ã¢y lÃ  quÃ¡ trÃ¬nh mÃ  mÃ´ hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng: 

![ldm](/assets/img/paper%20explained%206/stable%20diffusion.png)
_Stable Diffusion Process_

## 4. CLIP
Trong hÃ¬nh trÃªn, má»i ngÆ°á»i Ä‘á»ƒ Ã½ sáº½ tháº¥y má»™t layer cÃ³ tÃªn lÃ  `Text Encoder`, layer nÃ y sáº½ lÃ m nhiá»‡m vá»¥ chuyá»ƒn vÄƒn báº£n Ä‘áº§u vÃ o tá»« dáº¡ng ngÃ´n ngá»¯ tá»± nhiÃªn sang dáº¡ng sá»‘ Ä‘á»ƒ mÃ´ hÃ¬nh hiá»ƒu Ä‘Æ°á»£c (mÃ´ hÃ¬nh khÃ´ng Ä‘á»c Ä‘Æ°á»£c chá»¯ má»i ngÆ°á»i Ã , nÃ³ Ä‘á»c Ä‘Æ°á»£c sá»‘ thÃ´i). VÃ  trong bÃ i bÃ¡o gá»‘c, cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng mÃ´ hÃ¬nh `BERT` Ä‘á»ƒ lÃ m cÃ¡i layer nÃ y, nhÆ°ng phiÃªn báº£n Ä‘áº§u cá»§a Stable Diffusion thÃ¬ há» láº¡i dÃ¹ng `CLIP` cá»§a OpenAI. 

MÃ¬nh sáº½ lÃªn má»™t bÃ i cá»¥ thá»ƒ vá» `CLIP` sau, nhÆ°ng má»i ngÆ°á»i cÃ³ thá»ƒ Ä‘á»c tá»« chÃ­nh tÃ¡c giáº£ cá»§a bÃ i bÃ¡o nÃ y Ä‘á»ƒ hiá»ƒu Ä‘Æ°á»£c nÃ³ lÃ m gÃ¬, Ä‘Ã¢y lÃ  bÃ i [Ä‘Ã³](https://openai.com/research/clip). DÆ°á»›i Ä‘Ã¢y lÃ  tá»•ng quan mÃ´ hÃ¬nh nÃ y: 

![CLIP](/assets/img/paper%20explained%206/CLIP.png)
_CLIP Model_

CLIP (Contrastive Languageâ€“Image Pre-training) lÃ  má»™t mÃ´ hÃ¬nh **cho chÃºng ta biáº¿t kháº£ nÄƒng liÃªn káº¿t trá»±c tiáº¿p giá»¯a vÄƒn báº£n vÃ  hÃ¬nh áº£nh**, má»Ÿ ra nhiá»u á»©ng dá»¥ng cho cáº£ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  thá»‹ giÃ¡c mÃ¡y tÃ­nh. 

Má»¥c tiÃªu khi OpenAI huáº¥n luyá»‡n CLIP Ä‘á»ƒ nÃ³ trá»Ÿ thÃ nh má»™t pre-train model Ä‘Ã³ lÃ  há» sáº½ huáº¥n luyá»‡n pháº§n image encoder vÃ  text encoder Ä‘á»ƒ dá»± Ä‘oÃ¡n xem cÃ¡c áº£nh sáº½ Ä‘i chung vá»›i cÃ¢u nÃ o trong bá»™ dá»¯ liá»‡u cá»§a há». 

Vá»›i cÃ¡c lÃ½ do Ä‘Æ°á»£c nÃªu á»Ÿ trÃªn, CLIP khÃ´ng pháº£i Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t má»™t váº¥n Ä‘á» cá»¥ thá»ƒ nÃ o cáº£, mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n ngÃ¡ch khÃ¡c mÃ  váº«n dá»ƒ thá»Ÿ. 

## 5. Variational Autoencoder(VAE)
CÅ©ng trong hÃ¬nh mÃ  má»i ngÆ°á»i tháº¥y á»Ÿ trÃªn vá» cáº¥u trÃºc cá»§a mÃ´ hÃ¬nh stable diffusion, nÃ³ sáº½ cÃ³ 2 pháº§n Ä‘Ã³ lÃ  `Image Encoder` vÃ  `Image Decoder`. 

Quay trá»Ÿ láº¡i vá»›i lÃ­ do mÃ  nhÃ³m tÃ¡c giáº£ viáº¿t bÃ i nÃ y, cÃ³ thá»ƒ tháº¥y ráº±ng viá»‡c mÃ  há» denoise trÃªn má»™t latent variable cÃ³ kÃ­ch thÆ°á»›c báº±ng Ä‘Ãºng vá»›i kÃ­ch thÆ°á»›c Ä‘áº§u vÃ o sáº½ lÃ m cho viá»‡c denoise bá»©c áº£nh trá»Ÿ nÃªn ráº¥t tá»‘n kÃ©m, Ä‘áº·c biá»‡t lÃ  khi mÃ  kÃ­ch thÆ°á»›c bá»©c áº£nh lá»›n vÃ  bÆ°á»›c timestep $T$ lÃ  ráº¥t lá»›n. LÃºc nÃ y há» má»›i nghÄ© tá»›i viá»‡c **thay vÃ¬ há»c phÃ¢n phá»‘i $p(x)$ tá»« chÃ­nh dá»¯ liá»‡u hÃ¬nh áº£nh, thay vÃ o Ä‘Ã³, ta nÃªn há»c phÃ¢n phá»‘i cá»§a latent variable trÃªn táº­p dá»¯ liá»‡u, sá»­ dá»¥ng VAE**. 

![VAE](/assets/img/paper%20explained%206/VAE.jpg)
_VAE architecture_

Báº±ng cÃ¡ch Ã¡p dá»¥ng VAE Ä‘á»ƒ giáº£m sá»‘ chiá»u dá»¯ liá»‡u xuá»‘ng, chÃºng ta trá»±c tiáº¿p lÃ m giáº£m kÃ­ch thÆ°á»›c ma tráº­n Ä‘á»ƒ cho quÃ¡ trÃ¬nh denoise, cá»¥ thá»ƒ lÃ  tá»« má»™t bá»©c áº£nh `512 x 512` xuá»‘ng thÃ nh má»™t ma tráº­n `64 x 64` .Váº­y cÃ¢u há»i á»Ÿ Ä‘Ã¢y lÃ : "*Táº¡i sao VAE? ğŸ¤¨ AE thÆ°á»ng thÃ¬ sao khÃ´ng Ä‘Æ°á»£c?*". 

RÃµ rÃ ng Ä‘á»‘i vá»›i objective cá»§a chÃºng ta, Autoencoder cÅ©ng lÃ  má»™t lá»±a chá»n há»£p lÃ­, tuy nhiÃªn cÃ³ lÃ­ do Ä‘á»ƒ chÃºng ta khÃ´ng sá»­ dá»¥ng Autoencoder. NguyÃªn nhÃ¢n chÃ­nh yáº¿u nháº¥t Ä‘á»ƒ chÃºng ta khÃ´ng sá»­ dá»¥ng Autoencoder Ä‘Ã³ lÃ  **biá»ƒu diá»…n trong khÃ´ng gian tiá»m áº©n (latent space) khÃ´ng cÃ³ mang Ã½ nghÄ©a gÃ¬ cáº£**. MÃ´ hÃ¬nh cá»§a chÃºng ta sáº½ chá»‰ gÃ¡n cho cÃ¡i áº£nh Ä‘áº§u vÃ o má»™t vector input ngáº«u nhiÃªn (khÃ´ng háº³n lÃ  ngáº«u nhiÃªn, nhÆ°ng mÃ  váº¥n Ä‘á» lÃ  Autoencoder khÃ´ng cÃ³ há»c Ä‘Æ°á»£c má»‘i quan há»‡ cÃ³ nghÄ©a nÃ o giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cáº£, trong chÃ­nh cÃ¡i objective cá»§a AE lÃ  nÃ³ cá»‘ gáº¯ng tÃ¡i hiá»‡n láº¡i input cá»§a nÃ³ mÃ , kiá»ƒu nhÆ° mÃ´ hÃ¬nh nÃ y chá»‰ biáº¿t tá»›i báº£n thÃ¢n cá»§a nÃ³ thÃ´i ğŸ˜»ğŸ‘Œ). DÆ°á»›i Ä‘Ã¢y lÃ  kiáº¿n trÃºc cá»§a má»™t máº¡ng AE truyá»n thá»‘ng: 

![AE](/assets/img/paper%20explained%206/autoencoder.png)
_AE architecture_

Äiá»ƒm khÃ¡c nhau cá»§a 2 mÃ´ hÃ¬nh VAE vÃ  AE náº±m á»Ÿ chá»— VAE tháº­t sá»± cho phÃ©p chÃºng ta há»c ra má»™t latent space cÃ³ Ã½ nghÄ©a.

## 6. Latent Space
Váº­y cÃ¢u há»i káº¿ tiáº¿p (khÃ´ng quÃ¡ liÃªn quan Ä‘áº¿n bÃ i nÃ y, nhÆ°ng biáº¿t thÃ¬ cÅ©ng khÃ´ng háº¡i gÃ¬): *"Latent Space lÃ  gÃ¬ ğŸ˜?"*

Hiá»ƒu Ä‘Æ¡n giáº£n, Latent Space (hay khÃ´ng gian áº©n) thÆ°á»ng lÃ  má»™t khÃ´ng gian cÃ³ sá»‘ chiá»u tháº¥p hÆ¡n chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  váº«n giá»¯ Ä‘Æ°á»£c Ä‘áº·c tÃ­nh cá»§a dá»¯ liá»‡u input Ä‘Ã³, vÃ  chÃºng ta cÃ³ thá»ƒ lÃ m Ä‘Æ°á»£c viá»‡c nÃ y thÃ´ng qua cÃ¡c mÃ´ hÃ¬nh nhÆ° VAE hay AE. 

![mnist_latent](/assets/img/paper%20explained%206/mnist-latent.png)
_Latent space of MNIST_

NhÆ° trong hÃ¬nh nÃ y lÃ  má»™t latent space cÃ³ Ã½ nghÄ©a, cÃ¡c vector biá»ƒu diá»…n sau khi Ä‘Æ°á»£c nÃ©n vÃ o trong latent space sáº½ náº±m gáº§n nhau, ngá»¥ Ã½ ráº±ng cÃ¡c sample gáº§n giá»‘ng nhau thÃ¬ sáº½ náº±m gáº§n nhau trong má»™t khÃ´ng gian áº©n. Do Ä‘Ã³ khi chÃºng ta thá»±c hiá»‡n láº¥y máº«u, náº¿u mÃ  máº«u chÃºng ta láº¥y thuá»™c vÃ o má»™t cá»¥m nÃ o Ä‘Ã³, thÃ¬ kháº£ nÄƒng ráº¥t cao chÃºng ta sáº½ tÃ¡i táº¡o láº¡i Ä‘Æ°á»£c áº£nh ráº¥t liÃªn quan Ä‘áº¿n cá»¥m Ä‘Ã³. 

## 7. Denoising UNET 

Vai trÃ² cá»§a UNET trong mÃ´ hÃ¬nh stable diffusion lÃ  ráº¥t quan trá»ng (vÃ  cÃ³ váº» nhÆ° mÃ´ hÃ¬nh nÃ o thuá»™c dáº¡ng Diffusion-based cÅ©ng sá»­ dá»¥ng UNET Ä‘á»ƒ denoise). CÃ³ nhiá»u bÃ i khÃ¡c Ä‘Ã£ nÃ³i vá» UNET mÃ  má»i ngÆ°á»i cÃ³ thá»ƒ tÃ¬m Ä‘á»c(hoáº·c Ä‘á»£i mÃ¬nh lÃªn bÃ i ğŸ¥¹). LÃ½ do mÃ  mÃ´ hÃ¬nh cÃ³ tÃªn UNET lÃ  bá»Ÿi vÃ¬ cáº¥u trÃºc cá»§a nÃ³ táº¡o thÃ nh hÃ¬nh chá»¯ U: 

![UNET](/assets/img/paper%20explained%206/UNET.png)
_UNET architecture_

VÃ  á»Ÿ má»™t phÆ°Æ¡ng diá»‡n nÃ o Ä‘Ã³, Ä‘Ã¢y cÅ©ng chÃ­nh lÃ  má»™t dáº¡ng Autoencoder! 

NhÆ°ng mÃ  náº¿u chá»‰ cÃ³ váº­y thÃ¬ Ä‘iá»u gÃ¬ Ä‘Ã£ khiáº¿n UNET trá»Ÿ nÃªn Ä‘áº·c biá»‡t hÆ¡n? CÃ¢u tráº£ lá»i náº±m á»Ÿ nhá»¯ng Ä‘Æ°á»ng mÃ u xÃ¡m á»Ÿ hÃ¬nh trÃªn. Nhá»¯ng Ä‘Æ°á»ng xÃ¡m nÃ y chÃ­nh lÃ  nhá»¯ng káº¿t ná»‘i táº¯c (Residual Connection). 

Nhá»¯ng Ä‘Æ°á»ng Residual Connection nÃ y, hiá»ƒu Ä‘Æ¡n giáº£n lÃ  má»™t cÃ¡i cheatcode, vai trÃ² cá»§a nhá»¯ng Ä‘Æ°á»ng káº¿t ná»‘i nÃ y lÃ  há»— trá»£ ná»­a sau cá»§a pháº§n Decoder trong máº¡ng UNET, cá»¥ thá»ƒ hÆ¡n lÃ  á»Ÿ giai Ä‘oáº¡n Encode, á»Ÿ nhá»¯ng bÆ°á»›c Ä‘áº§u tiÃªn, chÃºng ta tháº­t sá»± cÃ³ Ä‘áº§y Ä‘á»§ thÃ´ng tin cá»§a bá»©c áº£nh Ä‘áº§u vÃ o, cÃ³ thá»ƒ nÃ³i á»Ÿ nhá»¯ng bÆ°á»›c nÃ y, mÃ´ hÃ¬nh pháº§n nÃ o Ä‘Ã³ báº£o toÃ n nhá»¯ng Ä‘áº·c trÆ°ng cá»§a áº£nh. NhÆ°ng váº¥n Ä‘á» náº±m á»Ÿ ná»­a sau, pháº§n Decode cá»‘ gáº¯ng phÃ³ng lá»›n láº¡i cÃ¡i áº£nh Ä‘ang Ä‘Æ°á»£c biá»ƒu diá»…n trong latent space, váº¥n Ä‘á» lÃ  lÃ m sao Ä‘á»ƒ pháº§n Decode biáº¿t mÃ¬nh Ä‘i Ä‘Ãºng hÆ°á»›ng? Nhá»¯ng cÃ¡i káº¿t ná»‘i táº¯c nÃ y sáº½ giÃºp pháº§n Decode biáº¿t lÃ  á»Ÿ layer Ä‘Ã³ thÃ¬ biá»ƒu diá»…n ma tráº­n nÃªn lÃ  nhÆ° tháº¿ nÃ o, ra sao. Tá»« Ä‘Ã³ nÃ³ khÃ´ng Ä‘i láº¡c ná»¯a. 

Fun fact ğŸ¤“â˜ï¸: UNET ban Ä‘áº§u Ä‘Æ°á»£c thiáº¿t káº¿ cho bÃ i toÃ¡n segmentation, nhÆ°ng do nhá»¯ng tÃ­nh cháº¥t mÃ  mÃ¬nh vá»«a nÃªu, má»i ngÆ°á»i Ä‘Ã£ mang nÃ³ vÃ´ háº§u háº¿t cÃ¡c bÃ i toÃ¡n táº¡o ra áº£nh cho cÃ¡c mÃ´ hÃ¬nh diffusion-base. 

## 8. Transformer 
CÅ©ng Ä‘Ã£ cÃ³ nhiá»u bÃ i post viáº¿t chi tiáº¿t vÃ  cá»¥ thá»ƒ vá» mÃ´ hÃ¬nh Transformer rá»“i, má»i ngÆ°á»i cÃ³ thá»ƒ tÃ¬m Ä‘á»c, trong khuÃ´n khá»• bÃ i toÃ¡n nÃ y, mÃ¬nh sáº½ nÃ³i váº¯ng táº¯t vá» cross-attention trong Transformer vÃ  cÃ¡ch nÃ³ Ä‘Æ°á»£c Ã¡p dá»¥ng trong bÃ i nÃ y.

**Vai trÃ² cá»§a cÆ¡ cháº¿ cross-attention**: Hiá»ƒu Ä‘Æ¡n giáº£n, tÆ°á»Ÿng tÆ°á»£ng báº¡n Ä‘ang Ä‘á»c má»™t bá»©c tranh, vÃ  Ä‘á»“ng thá»i Ä‘á»c luÃ´n caption cá»§a bá»©c tranh Ä‘Ã³. LÃºc nÃ y cÃ³ thá»ƒ báº¡n Ä‘ang so sÃ¡nh Ä‘á»‘i tÆ°á»£ng trong tranh vá»›i cÃ¡c tá»« báº¡n quan sÃ¡t Ä‘Æ°á»£c trong cÃ¢u mÃ´ táº£ Ä‘á»ƒ xem nÃ³ cÃ³ trÃ¹ng khÃ´ng. 


# CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng

Ok, sau khi Ä‘Ã£ cÃ³ hiá»ƒu biáº¿t vá» cÃ¡c thÃ nh pháº§n trong mÃ´ hÃ¬nh Stable Diffusion, giá» ta sáº½ Ä‘i Ä‘áº¿n pháº§n chÃ­nh, Ä‘Ã³ lÃ  cáº¥u trÃºc cá»¥ thá»ƒ cá»§a mÃ´ hÃ¬nh, má»i ngÆ°á»i cÃ³ thá»ƒ nhÃ¬n vÃ o hÃ¬nh dÆ°á»›i Ä‘Ã¢y: 

![stable_diffusion_full](/assets/img/paper%20explained%206/stable_diffusion_full.png)
_Full Component of Stable Diffusion Architecture_

á» Ä‘Ã¢y, cÃ³ má»™t Ä‘iá»ƒm lÆ°u Ã½ Ä‘Ã³ lÃ  pháº§n Encoder $\tau_\theta$. á» trÃªn mÃ¬nh cÃ³ nÃ³i cá»¥ thá»ƒ cÃ¡i Ä‘Ã³ lÃ  pháº§n Text Encoder, nhÆ°ng trong paper gá»‘c thÃ¬ há» láº¡i ghi lÃ  $\tau_\theta$, lÃ½ do cho viá»‡c nÃ y náº±m á»Ÿ chá»— pháº§n Text tháº­t ra lÃ  má»™t trong nhá»¯ng Ä‘iá»u kiá»‡n giÃºp chÃºng ta Ä‘iá»u chá»‰nh láº¡i quÃ¡ trÃ¬nh denoise. NhÆ°ng ngoÃ i tháº­t ra, ngoÃ i Text, chÃºng ta cÃ³ thá»ƒ thay Ä‘á»•i Ä‘iá»u kiá»‡n Ä‘Ã³ thÃ nh nhá»¯ng Ä‘iá»u kiá»‡n khÃ¡c sao cho phÃ¹ há»£p vá»›i task cá»§a chÃºng ta. Láº¥y vÃ­ dá»¥ nhÆ° vá»›i bÃ i toÃ¡n tÃ´ mÃ u áº£nh, mÃ¬nh hoÃ n toÃ n cÃ³ thá»ƒ thiáº¿t káº¿ láº¡i mÃ´ hÃ¬nh nÃ y sao cho nÃ³ nháº­n vÃ o 2 bá»©c áº£nh, má»™t bá»©c áº£nh Ä‘Ã³ng vai trÃ² input, má»™t bá»©c áº£nh Ä‘Ã³ng vai trÃ² lÃ  Ä‘iá»u kiá»‡n Ä‘á»ƒ Ä‘iá»u chá»‰nh cho phÃ¹ há»£p vá»›i Ã½ muá»‘n cá»§a mÃ¬nh. **CÃ³ thá»ƒ nÃ³i, Stable Diffusion bÃªn cáº¡nh giáº£i quyáº¿t váº¥n Ä‘á» vá» tÃ­nh toÃ¡n cá»§a Diffusion model, mÃ´ hÃ¬nh nÃ y cÃ²n cho phÃ©p Ä‘a dáº¡ng dá»¯ liá»‡u Ä‘áº§u vÃ o, nhá» vÃ o viá»‡c pháº§n conditioning cÃ³ thá»ƒ Ä‘a dáº¡ng** 


# Code 

# Tháº£o luáº­n káº¿t quáº£ 

