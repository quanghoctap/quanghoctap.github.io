---
title: 'Paper Explained 1: Convolutional Neural Network for Sentence Classification'
date: 2024-01-21
categories: [Data Science, Deep Learning]
tags: [cnn, paper explained]
toc: true
math: true
publish: true
---

ChÃ o má»i ngÆ°á»i, á»Ÿ trong sá»‘ Ä‘áº§u tiÃªn, chÃºng ta sáº½ tÃ¬m hiá»ƒu vá» cÃ¡ch thá»©c mÃ  má»™t máº¡ng CNN (vá»‘n ráº¥t ná»•i tiáº¿ng trong cÃ¡c bÃ i toÃ¡n xá»­ lÃ­ áº£nh) láº¡i cÃ³ thá»ƒ Ã¡p dá»¥ng Ä‘Æ°á»£c vÃ o trong bÃ i toÃ¡n xá»­ lÃ­ ngÃ´n ngá»¯ tá»± nhiÃªn, mÃ  á»Ÿ Ä‘Ã¢y cá»¥ thá»ƒ lÃ  bÃ i toÃ¡n phÃ¢n loáº¡i cÃ¢u mÃ  váº«n cho ra má»™t káº¿t quáº£ tá»‘t. VÃ  cÅ©ng nhÆ° trong bÃ i nÃ y, chÃºng ta sáº½ bÃ n luáº­n thÃªm vá» cÃ¡c Æ°u nhÆ°á»£c Ä‘iá»ƒm cÅ©ng nhÆ° cÃ¡c cÃ¡ch khÃ¡c mÃ  ta cÃ³ thá»ƒ cáº£i tiáº¿n Ä‘Æ°á»£c káº¿t quáº£ cá»§a mÃ´ hÃ¬nh nÃ y.  

Má»i ngÆ°á»i cÃ³ thá»ƒ Ä‘á»c bÃ i bÃ¡o gá»‘c á»Ÿ [Ä‘Ã¢y](https://arxiv.org/pdf/1408.5882.pdf).

# Giá»›i thiá»‡u
Äá»ƒ cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c bÃ i viáº¿t nÃ y, sáº½ cÃ³ má»™t vÃ i khÃ¡i niá»‡m mÃ  má»i ngÆ°á»i sáº½ cáº§n pháº£i biáº¿t trÆ°á»›c Ä‘á»ƒ khÃ´ng bá»‹ "á»§a lÃ  sao ta??!" (biáº¿t trÆ°á»›c á»Ÿ Ä‘Ã¢y lÃ  má»i ngÆ°á»i biáº¿t Ä‘Æ°á»£c cÃ¡i cÃ´ng dá»¥ng cá»§a nÃ³ lÃ  Ä‘á»§ Ä‘á»ƒ hiá»ƒu rá»“i). CÃ¡c khÃ¡i niá»‡m Ä‘Ã³ sáº½ bao gá»“m lá»›p [embedding](https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16), [convolution](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1), [pooling](https://www.youtube.com/watch?v=8oOgPUO-TBY), [linear](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjTopqI8duDAxX7k1YBHUl3BdYQFnoECAoQAQ&url=https%3A%2F%2Fmedium.com%2Fdatathings%2Flinear-layers-explained-in-a-simple-way-2319a9c2d1aa&usg=AOvVaw1vjNqtJwb0tZp_vA196sc4&opi=89978449). NhÆ°ng khÃ´ng sao, má»i ngÆ°á»i cá»© báº¥m vÃ o link trong má»—i tá»«, mÃ¬nh tháº¥y máº¥y cÃ¡i trang nÃ y giáº£i thÃ­ch cÅ©ng ok.

**!!! Clarify !!!**: Ban Ä‘áº§u lÃ  máº¥y máº¡ng CNN dÃ¹ng Ä‘á»ƒ xá»­ lÃ½ áº£nh, nhÆ°ng mÃ  cÅ©ng cÃ³ vÃ i bÃ i nghiÃªn cá»©u mang CNN ra khá»i khuÃ´n khá»• cá»§a xá»­ lÃ­ áº£nh, há» chuyá»ƒn nÃ³ sang Ä‘á»ƒ giáº£i quyáº¿t máº¥y bÃ i toÃ¡n bÃªn NLP nhÆ° lÃ  semantic matching hay lÃ  search query retrieval hay lÃ  nhá»¯ng bÃ i toÃ¡n NLP cÆ¡ báº£n khÃ¡c. á» Ä‘Ã¢y cÃ³ má»™t Ä‘iá»ƒm ráº¥t thÃº vá»‹ lÃ  khi nháº¯c Ä‘áº¿n máº¡ng CNN cho chá»¯ thÃ¬ má»i ngÆ°á»i nghÄ© ngay Ä‘áº¿n cÃ¡i bÃ i mÃ  mÃ¬nh vá»«a link á»Ÿ trÃªn, nhÆ°ng tháº­t ra cÃ³ má»™t bÃ i viáº¿t ná»n táº£ng cho cÃ¡i nÃ y mÃ  mÃ¬nh nghÄ© má»i ngÆ°á»i cÅ©ng nÃªn quan tÃ¢m Ä‘Ã³ lÃ  bÃ i ["A Convolutional Neural Network for Modelling Sentences"](https://arxiv.org/pdf/1404.2188.pdf). CÅ©ng nhÆ° lÃ  má»™t bÃ i khÃ¡c mÃ  mÃ´ mÃ¬nh nÃ y láº¥y cáº£m há»©ng Ä‘Ã³ lÃ  bÃ i ["Natural Language Processing (almost) from Scratch"](https://arxiv.org/pdf/1103.0398.pdf). MÃ¬nh nghÄ© hai bÃ i nÃ y lÃ  ná»n táº£ng cho cÃ¡i bÃ i mÃ  tá»¥i mÃ¬nh phÃ¢n tÃ­ch ngÃ y hÃ´m nay

# CÃ¡ch máº¡ng TextCNN hoáº¡t Ä‘á»™ng
## Biá»ƒu diá»…n chá»¯ dÆ°á»›i dáº¡ng vector
Äáº§u tiÃªn chÃºng ta cáº§n hiá»ƒu word embedding lÃ  gÃ¬, náº¿u nhÆ° cÃ¡c báº¡n Ä‘Ã£ hiá»ƒu cá»¥ thá»ƒ nÃ³ lÃ  gÃ¬ (hoáº·c chÆ°a hiá»ƒu thÃ¬ link vá» embedding á»Ÿ trÃªn lÃ  má»™t nguá»“n Ä‘á»c ráº¥t tá»‘t) thÃ¬ tÃ³m gá»n láº¡i lÃ  viá»‡c chÃºng ta biá»ƒu diá»…n má»™t chá»¯ dÆ°á»›i dáº¡ng vector. NhÆ° hÃ¬nh váº½ sau: 

![word embedding](/assets/img/blog1/word_embedding.jpg)

NhÆ° trong hÃ¬nh váº½ trÃªn thÃ¬ má»™t chá»¯ cá»§a chÃºng ta sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n thÃ nh má»™t vector cÃ³ 7 pháº§n tá»­ (sá»‘ chiá»u lÃ  7). Tháº­t ra thÃ¬ sá»‘ chiá»u nÃ y lÃ  bao nhiÃªu cÅ©ng Ä‘Æ°á»£c, Ä‘Ã¢y lÃ  má»™t siÃªu tham sá»‘ mÃ  má»i ngÆ°á»i cÃ³ thá»ƒ chá»n, vá»›i cÃ¡ch nghÄ© thÃ´ng thÆ°á»ng thÃ¬ vá»›i má»™t sá»‘ chiá»u cÃ ng lá»›n, cÃ¡c tá»« cá»§a chÃºng ta Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi má»™t vector dÃ i hÆ¡n, do Ä‘Ã³ mÃ  náº¯m báº¯t Ä‘Æ°á»£c nhiá»u ngá»¯ nghÄ©a cá»§a tá»« hÆ¡n, vÃ  cÅ©ng nhÆ° váº­y, má»™t sá»‘ chiá»u dÃ i hÆ¡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i chi phÃ­ tÃ­nh toÃ¡n cao hÆ¡n. VÃ  cÅ©ng trong hÃ¬nh váº½ nÃ y, náº¿u nhÆ° cÃ¡c chá»¯ Ä‘Æ°á»£c biá»ƒu diá»…n tá»‘t (biá»ƒu diá»…n tá»‘t á»Ÿ Ä‘Ã¢y lÃ  cÃ¡c chá»¯ cÃ¹ng liÃªn quan tá»›i má»™t nghÄ©a hay cÃ³ cÃ¡i sá»± liÃªn quan mÃ  má»i ngÆ°á»i Ä‘á»c qua tháº¥y cÅ©ng há»£p lÃ­) thÃ¬ sáº½ cÃ³ khoáº£ng cÃ¡ch ngáº¯n, hay hiá»ƒu lÃ  chÃºng náº±m gáº§n nhau cÅ©ng Ä‘Æ°á»£c, giá»‘ng nhÆ° 'puppy' vá»›i 'dog' cÃ¹ng cÃ³ nghÄ©a lÃ  con chÃ³, kiá»ƒu kiá»ƒu váº­y ğŸ¥². 

Ok váº­y cÃ¢u há»i tiáº¿p theo lÃ  giÃ¡ trá»‹ cá»§a máº¥y pháº§n tá»­ nÃ y ngÆ°á»i ta láº¥y á»Ÿ Ä‘Ã¢u? ThÃ¬ cÃ¢u tráº£ lá»i lÃ  ngÆ°á»i ta sáº½ random cho má»i ngÆ°á»i nha, random trong khoáº£ng tá»« -1 tá»›i 1 trong trÆ°á»ng há»£p cá»§a hÃ¬nh váº½. CÃ²n trong bÃ i nghiÃªn cá»©u mÃ  chÃºng ta tÃ¬m hiá»ƒu thÃ¬ cÃ¡c biá»ƒu diá»…n cá»§a tá»« nÃ y Ä‘Æ°á»£c láº¥y tá»« má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ há»c khÃ´ng giÃ¡m sÃ¡t trÃªn táº­p dá»¯ liá»‡u Google News cá»§a [Mikolov et al](https://arxiv.org/pdf/1301.3781.pdf). 

CÃ¢u há»i tiáº¿p theo lÃ  táº¡i sao pháº£i lÃ m váº­y? ThÃ¬ cÃ¢u tráº£ lá»i Ä‘Æ¡n giáº£n lÃ  100 tá»· tá»« Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u Google News Ä‘Ã£ lÃ  cÃ¡c chá»¯ Ä‘Æ°á»£c biá»ƒu diá»…n tá»‘t nhÆ° mÃ¬nh Ä‘á» cáº­p á»Ÿ trÃªn, vÃ  hÆ¡n ná»¯a, lÃ m nhÆ° váº­y sáº½ tá»‘i Æ°u hÆ¡n viá»‡c mÃ  chÃºng ta chá»‰ táº¡o ngáº«u nhiÃªn giÃ¡ trá»‹ cho tá»«ng pháº§n tá»­ cá»§a trong vector cá»§a má»—i tá»« (hiá»ƒu nÃ´m na lÃ  ngáº«u nhiÃªn thÃ´i thÃ¬ máº¥y cÃ¡i tá»« tá»± nhiÃªn cÃ³ Ã½ nghÄ©a gÃ¬ Ä‘Ã¢u ğŸ¤¨).

## Vá» mÃ´ hÃ¬nh
Cáº¥u trÃºc mÃ´ hÃ¬nh nÃ y khÃ¡ Ä‘Æ¡n giáº£n, chÃºng ta sáº½ xem tá»•ng quan mÃ´ hÃ¬nh ra sao rá»“i sau Ä‘Ã³ Ä‘Ã o sÃ¢u hÆ¡n vá» máº·t toÃ¡n há»c cÅ©ng nhÆ° cÃ¡ch mÃ  mÃ´ hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng, cÃ¡c cÃ¡ch biá»ƒu diá»…n Ä‘áº§u ra, Ä‘áº§u vÃ o lÃ  nhÆ° tháº¿ nÃ o,v.v.... DÆ°á»›i Ä‘Ã¢y lÃ  cáº¥u trÃºc mÃ´ hÃ¬nh:

![model architecture](/assets/img/blog1/model_structure.png)

HÃ¬nh trÃªn Ä‘Æ°á»£c láº¥y tá»« bÃ i paper tá»¥i mÃ¬nh sáº½ nghiÃªn cá»©u, nÃªn lÃ , tin chuáº©n nháº¿ má»i ngÆ°á»i ğŸ»
## Biá»ƒu diá»…n Ä‘áº§u vÃ o
NhÆ° há»“i nÃ£y mÃ¬nh cÃ³ nÃ³i lÃ  cÃ¡c chá»¯ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng vector, mÃ  cá»¥ thá»ƒ hÆ¡n thÃ¬ cÃ³ dáº¡ng $\mathbf{x}_i \in \mathbb{R}^k$ vá»›i i lÃ  thá»© tá»± cá»§a chá»¯ Ä‘Ã³ trong cÃ¢u, cÃ²n k lÃ  sá»‘ chiá»u cá»§a vector thá»© i Ä‘Ã³. 

Náº¿u má»™t tá»« Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t vector nhÆ° váº­y thÃ¬ á»Ÿ Ä‘Ã¢y má»™t cÃ¢u sáº½ biá»ƒu diá»…n dÆ°á»›i dáº¡ng cá»§a ...ma tráº­n (rÃµ rÃ ng luÃ´n ğŸ˜ğŸ‘Œ). Cá»¥ thá»ƒ hÆ¡n thÃ¬ má»™t cÃ¢u cÃ³ Ä‘á»™ dÃ i lÃ  n tá»« thÃ¬ sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau: 

$$
\mathbf{x}_{(1:n)} = \mathbf{x}_1 \oplus \mathbf{x}_2 \oplus \mathbf{x}_3 \oplus \ldots \oplus \mathbf{x}_n
$$

Trong Ä‘Ã³ thÃ¬ $\oplus$ lÃ  phÃ©p toÃ¡n concatenation (phÃ©p toÃ¡n ghÃ©p) vÃ  Ä‘á»ƒ khÃ¡i quÃ¡t hÆ¡n cho trÆ°á»ng há»£p cá»§a má»™t cÃ¢u, láº¥y vÃ­ dá»¥ ta khÃ´ng quan tÃ¢m tá»›i toÃ n bá»™ cÃ¢u, mÃ  chá»‰ má»™t Ä‘oáº¡n nhá» trong cÃ¢u thÃ´i thÃ¬ ta cÅ©ng cÃ³ thá»ƒ biá»ƒu diá»…n dÆ°á»›i cÃ´ng thá»©c toÃ¡n nhÆ° sau: 

$$
\mathbf{x}_{(i:j)} = \mathbf{x}_i \oplus \mathbf{x}_{i+1} \oplus \mathbf{x}_{i+2} \oplus \ldots \oplus \mathbf{x}_j
$$ 

Giá» láº¥y vÃ­ dá»¥ mÃ¬nh cÃ³ má»™t cÃ¢u hoÃ n chá»‰nh lÃ : `I think your shirts look nice, i like it` thÃ¬ mÃ¬nh chá»‰ chá»n ra tá»« Ä‘oáº¡n `your` tá»›i Ä‘oáº¡n `nice` thÃ´i, náº¿u mÃ¬nh lÃ m nhÆ° váº­y thÃ¬ mÃ¬nh sáº½ cÃ³ má»™t ma tráº­n nhÆ° bÃªn dÆ°á»›i:

![matrix representation](/assets/img/blog1/matrix_sentence.png)

## Convolution (tÃ­ch cháº­p) vÃ  output (Ä‘áº§u ra)
NhÆ° váº­y thÃ¬ giá» chÃºng ta Ä‘Ã£ hiá»ƒu Ä‘áº§u vÃ o cá»§a thuáº­t toÃ¡n sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng cá»§a má»™t ma tráº­n `n x k`. Giá» mÃ¬nh nÃ³i tiáº¿p phÃ©p convolution cÅ©ng nhÆ° lÃ  cÃ¡ch mÃ  chÃºng ta cÃ³ Ä‘Æ°á»£c Ä‘áº§u ra cá»§a phÃ©p convolution. 

CÃ³ láº½ má»i ngÆ°á»i cÅ©ng Ä‘Ã£ hiá»ƒu cÃ¡i filter mÃ  ngÆ°á»i ta thÆ°á»ng nÃ³i trong cÃ¡c phÃ©p convolution lÃ  gÃ¬ (náº¿u chÆ°a thÃ¬ xem cÃ¡i link á»Ÿ Ä‘áº§u nha ğŸ¥¹). ThÃ¬ vá»›i mÃ´ hÃ¬nh hiá»‡n giá» cá»§a chÃºng ta, cÃ¡i filter Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a má»™t ma tráº­n nhÆ°: $\mathbf{w} \in \mathbb{R}^{h \times k}$ vá»›i h lÃ  sá»‘ tá»«, cÃ²n k lÃ  chiá»u cá»§a vector biá»ƒu diá»…n tá»« nhÆ° há»“i nÃ£y mÃ¬nh cÃ³ nÃ³i. 

Vá»›i nhá»¯ng cÃ¡i thÃ´ng tin mÃ¬nh cung cáº¥p vá» Ä‘áº§u vÃ o vÃ  filter. Filter mÃ  mÃ¬nh vá»«a Ä‘á» cáº­p sáº½ thá»±c hiá»‡n phÃ©p convolution vá»›i má»™t ma tráº­n h tá»« Ä‘á»ƒ Ä‘Æ°a ra má»™t feature (thuá»™c tÃ­nh) má»›i, láº¥y vÃ­ dá»¥ lÃ  $c_{i}$ Ä‘Æ°á»£c tÃ­nh nhÆ° sau:

$$
c_{i} = \sigma(\mathbf{w} * \mathbf{x}_{(i:i+h)} + b)
$$

á» Ä‘Ã¢y, $\sigma$ lÃ  má»™t hÃ m phi tuyáº¿n nÃ o Ä‘Ã³, láº¥y vÃ­ dá»¥ nhÆ° lÃ  hÃ m `tanh` hoáº·c lÃ  hÃ m `ReLU`, b lÃ  bias (khÃ´ng biáº¿t dá»‹ch cÃ¡i nÃ y kiá»ƒu gÃ¬ ğŸ¤·â€â™‚ï¸). VÃ  Ä‘Ã³ lÃ  cÃ¡ch mÃ  tá»« má»™t cÃ¡i filter, Ä‘áº§u vÃ o, mÃ  chÃºng ta ra Ä‘Æ°á»£c má»™t con sá»‘ Ä‘áº¡i diá»‡n cho cÃ¡i Ä‘oáº¡n h tá»« Ä‘Ã³. NhÆ°ng khÃ´ng chá»‰ dá»«ng láº¡i á»Ÿ Ä‘Ã³, ta cÃ³ thá»ƒ Ã¡p cÃ¡i cá»¥c filter nÃ y vÃ o tá»«ng cá»­a sá»• h tá»« trong cÃ¢u sao cho nÃ³ duyá»‡t qua háº¿t táº¥t cáº£ cÃ¡c cá»­a sá»• h tá»« trong cÃ¢u vá»›i biá»ƒu diá»…n toÃ¡n há»c lÃ  
$$
\{\mathbf{x}_{(1:h)}, \mathbf{x}_{(2:h+1)}, \ldots, \mathbf{x}_{(n-h+1:n)}\}
$$
Ä‘á»ƒ táº¡o ra má»™t cÃ¡i feature map (biá»ƒu diá»…n Ä‘áº·c trÆ°ng) tÆ°Æ¡ng á»©ng cho cÃ¢u. VÃ  nhÆ° váº­y, chÃºng ta cÃ³ má»™t vector hoÃ n chá»‰nh Ä‘áº¡i diá»‡n cho cÃ¢u nhÆ° cÃ´ng thá»©c sau:

$$
\mathbf{c} = [c_1, c_2, \ldots, c_{n-h+1}]
$$

Sau khi Ä‘Ã£ cÃ³ vector $\mathbf{c}$ nÃ y rá»“i, chÃºng ta thá»±c hiá»‡n phÃ©p max pooling (gá»™p cá»±c Ä‘áº¡i) Ä‘á»ƒ láº¥y ra feature Ä‘áº¡i diá»‡n cho cÃ¡i filter cá»¥ thá»ƒ mÃ  chÃºng ta Ä‘ang xÃ©t nÃ y: 
$$
c_{\text{max}} = \max(\mathbf{c}).
$$
ChÃºng ta lÃ m váº­y bá»Ÿi vÃ¬ chÃºng ta tin ráº±ng Ä‘áº·c trÆ°ng quan trá»ng nháº¥t cá»§a má»—i feature map lÃ  Ä‘áº·c trÆ°ng cÃ³ giÃ¡ trá»‹ lá»›n nháº¥t.

Váº­y cÃ²n "má»—i feature map ?". "má»—i" lÃ  bá»Ÿi vÃ¬ chÃºng ta sáº½ khÃ´ng sá»­ dá»¥ng duy nháº¥t má»™t filter, mÃ  chÃºng ta sáº½ cÃ³ nhiá»u filter khÃ¡c nhau, vÃ  Ä‘áº·c Ä‘iá»ƒm cá»§a cÃ¡c filter nÃ y lÃ  chÃºng sáº½ cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau (khÃ¡c nhau á»Ÿ sá»‘ tá»« mÃ  chÃºng ta sáº½ láº¥y á»Ÿ tá»«ng filter). Nghe hÆ¡i láº±ng nháº±ng Ä‘Ãºng khÃ´ng? KhÃ´ng sao, hÃ¬nh dÆ°á»›i Ä‘Ã¢y sáº½ giáº£i thÃ­ch Ä‘Æ°á»£c cÃ¡i Ä‘Ã³:

![Model Structure 2](/assets/img/blog1/model_structure_2.png)

CÃ¡i hÃ¬nh nÃ y vá» cÆ¡ báº£n cÅ©ng lÃ  mÃ´ hÃ¬nh mÃ  chÃºng ta Ä‘ang xÃ¢y dá»±ng, lÃ  báº£n "nhiá»u mÃ u sáº¯c" so vá»›i mÃ´ hÃ¬nh mÃ  Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ trÃªn. Má»i ngÆ°á»i Ä‘á»ƒ Ã½ nÃ³ cÃ³ máº¥y cá»¥c filter nhiá»u mÃ u Ä‘Ã³ khÃ´ng? ÄÃ³ lÃ  cÃ¡i mÃ¬nh vá»«a nÃ³i, tá»©c lÃ  chÃºng ta sáº½ sá»­ dá»¥ng nhá»¯ng filter khÃ¡c nhau vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau (lÃ  máº¥y cá»¥c nhiá»u mÃ u Ä‘Ã³ Ä‘Ã³). 

Giá», á»©ng vá»›i má»—i filter khÃ¡c nhau, ta thá»±c hiá»‡n phÃ©p convolution giá»¯a má»—i filter Ä‘Ã³ vá»›i ma tráº­n Ä‘áº§u vÃ o, sáº½ cho ra cÃ¡c vector cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau, tÃ¹y thuá»™c vÃ o kÃ­ch cá»¡ cá»§a cÃ¡i filter, vÃ  sau Ä‘Ã³, thá»±c hiá»‡n max pooling, ta sáº½ thu Ä‘Æ°á»£c feature Ä‘áº¡i diá»‡n mÃ  lÃºc trÆ°á»›c Ä‘Æ°á»£c Ä‘á» cáº­p Ä‘áº¿n. 

**Váº­y Ã½ nghÄ©a cá»§a viá»‡c cáº§n cÃ¡c filter khÃ¡c nhau lÃ  gÃ¬?** CÃ¢u tráº£ lá»i lÃ  mÃ´ hÃ¬nh sáº½ sá»­ dá»¥ng nhiá»u filter khÃ¡c nhau Ä‘á»ƒ náº¯m báº¯t Ä‘Æ°uá»c nhiá»u thÃ´ng tin khÃ¡c nhau trong cÃ¢u (cÃ²n thÃ´ng tin Ä‘Ã³ lÃ  gÃ¬ thÃ¬ chÃºng ta chÆ°a biáº¿t Ä‘Æ°á»£c) theo nhÆ° tÃ¡c giáº£ giáº£i thÃ­ch. CÃ²n táº¡i sao láº¡i nhÆ° váº­y thÃ¬ hiá»‡n táº¡i chÆ°a cÃ³ cÃ¢u tráº£ lá»i, do Ä‘Ã³ cáº§n thÃªm cÃ¡c nghiÃªn cá»©u chuyÃªn sÃ¢u khÃ¡c vá» váº¥n Ä‘á» nÃ y. NhÆ°ng theo mÃ¬nh, viá»‡c chÃºng ta cÃ³ nhiá»u parameters hÆ¡n cÅ©ng Ä‘á»“ng nghÄ©a vá»›i viá»‡c mÃ´ hÃ¬nh trá»Ÿ nÃªn Ä‘a dáº¡ng hÆ¡n, cÃ²n vá» viá»‡c cÃ¡c filter sáº½ thay Ä‘á»•i nhÆ° tháº¿ nÃ o lÃ  phá»¥ thuá»™c vÃ o viá»‡c chÃºng ta xÃ¡c Ä‘á»‹nh cÃ¡i objective cá»§a chÃºng ta lÃ  gÃ¬. CÃ²n láº¡i, cá»© Ä‘á»ƒ mÃ´ hÃ¬nh lo ğŸ¤¡ğŸ¤¡ğŸ¤¡. BÃªn lá» lÃ  cÃ³ má»™t bÃ i viáº¿t cÅ©ng hay vá» chá»§ Ä‘á» nÃ y, má»i ngÆ°á»i cÃ³ thá»ƒ Ä‘á»c thÃªm á»Ÿ [Ä‘Ã¢y](https://towardsdatascience.com/why-we-will-never-open-deep-learnings-black-box-4c27cd335118)

## Perceptron Ä‘a táº§ng (multilayer perceptron)
NÃ³i nÃ´m na, Ä‘Ã¢y lÃ  má»™t loáº¡i máº¡ng nÆ¡-ron nhÃ¢n táº¡o (ANN) cÃ³ cáº¥u trÃºc gá»“m nhiá»u lá»›p áº©n, má»—i lá»›p chá»©a má»™t sá»‘ lÆ°á»£ng nÃºt nháº¥t Ä‘á»‹nh. VÃ  trong trÆ°á»ng há»£p cá»§a chÃºng ta thÃ¬ cÃ¡c feature cuá»‘i cÃ¹ng (sau khi Ä‘Ã£ qua giai Ä‘oáº¡n max pooling) sáº½ Ä‘Æ°á»£c ghÃ©p láº¡i vá»›i nhau Ä‘á»ƒ táº¡o ra má»™t vector Ä‘áº¡i diá»‡n cuá»‘i cÃ¹ng vá»›i má»—i pháº§n tá»­ trong vector lÃ  má»™t nÃºt. 

Vector Ä‘áº¡i diá»‡n nÃ y sáº½ Ä‘Æ°á»£c truyá»n qua lá»›p fully connected layer (káº¿t ná»‘i Ä‘áº§y Ä‘á»§) vÃ  sá»­ dá»¥ng hÃ m phi tuyáº¿n softmax Ä‘á»ƒ Ä‘Æ°a giÃ¡ trá»‹ Ä‘áº§u ra trá»Ÿ thÃ nh cÃ¡c con sá»‘ xÃ¡c suáº¥t cÃ³ tá»•ng báº±ng 1. 

## Regularization (cÆ¡ cháº¿ kiá»ƒm soÃ¡t)
Äá»ƒ Ä‘Æ°a ra má»™t cÃ¡i nhÃ¬n tá»•ng quÃ¡t á»Ÿ Ä‘oáº¡n nÃ y, mÃ¬nh xin trÃ­ch láº¡i má»™t Ä‘oáº¡n tá»« trong cuá»‘n sÃ¡ch ["Deep Learing"](https://imlab.postech.ac.kr/dkim/class/csed514_2019s/DeepLearningBook.pdf) cá»§a Ian Goodfellow vÃ  cá»™ng sá»±: **cÆ¡ cháº¿ kiá»ƒm soÃ¡t lÃ  "báº¥t ká»³ sá»± Ä‘iá»u chá»‰nh nÃ o trong thuáº­t toÃ¡n há»c táº­p nháº±m giáº£m sai sá»‘ tá»•ng quÃ¡t hÃ³a, khÃ´ng nháº±m giáº£m sai sá»‘ huáº¥n luyá»‡n".** CÃ³ nhiá»u cÆ¡ cháº¿ kiá»ƒm soÃ¡t khÃ¡c nhau, nhÆ°ng á»Ÿ Ä‘Ã¢y nhÃ³m tÃ¡c giáº£ sáº½ sá»­ dá»¥ng 2 cÆ¡ cháº¿ kiá»ƒm soÃ¡t chÃ­nh lÃ  **l2 regularization(cÆ¡ cháº¿ pháº¡t chuáº©n l2)** vÃ  **dropout(cÆ¡ cháº¿ táº¯t ngáº«u nhiÃªn)**

HÃ m pháº¡t chuáº©n $L^{2}$ cÃ²n Ä‘Æ°á»£c biáº¿t Ä‘áº¿n vá»›i tÃªn gá»i khÃ¡c lÃ  suy weight decay (suy giáº£m trá»ng sá»‘) vÃ  Ã½ nghÄ©a cá»§a nÃ³ hiá»ƒu nÃ´m na lÃ  nÃ³ sáº½ thÃªm má»™t Ä‘áº¡i lÆ°á»£ng kiá»ƒm soÃ¡t vÃ o objective function (hÃ m má»¥c tiÃªu) Ä‘á»ƒ hÆ°á»›ng trá»ng sá»‘ mÃ´ hÃ¬nh vá» gáº§n gá»‘c tá»a Ä‘á»™ (tháº­t ra lÃ  khÃ´ng vá» báº¥t kÃ¬ Ä‘iá»ƒm nÃ o trong khÃ´ng gian cÅ©ng Ä‘Æ°á»£c chá»© khÃ´ng nháº¥t thiáº¿t pháº£i lÃ  gá»‘c tá»a Ä‘á»™). Objective function sau khi káº¿t há»£p vá»›i l2 regularization sáº½ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° hÃ¬nh dÆ°á»›i Ä‘Ã¢y:

![l2 regularization](/assets/img/blog1/regularization.png)

Ã nghÄ©a cá»§a viá»‡c nÃ y (mÃ¬nh trÃ­ch láº¡i tá»« cuá»‘n sÃ¡ch vá» Ä‘á» cáº­p) lÃ : Bá»™ kiá»ƒm soÃ¡t $L^{2}$ khiáº¿n cho thuáº­t toÃ¡n há»c táº­p "nháº­n thá»©c" ráº±ng giÃ¡ trá»‹ Ä‘áº§u vÃ o $\mathbf{X}$ cÃ³ phÆ°Æ¡ng sai lá»›n hÆ¡n, khiáº¿n nÃ³ lÃ m co trá»ng sá»‘ cá»§a cÃ¡c Ä‘áº·c trÆ°ng cÃ³ hiá»‡p phÆ°Æ¡ng sai Ä‘á»‘i vá»›i nhÃ£n Ä‘áº§u ra cÃ³ giÃ¡ trá»‹ nhá» so vá»›i giÃ¡ trá»‹ phÆ°Æ¡ng sai má»›i. 

Vá» dropout, nÃ³i nÃ´m lÃ , cÆ¡ cháº¿ nÃ y há»— trá»£ huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng cÃ¡ch loáº¡i bá» cÃ¡c hidden units(Ä‘Æ¡n vá»‹ áº©n), Ä‘iá»u nÃ y buá»™c mÃ´ hÃ¬nh pháº£i há»c cÃ¡ch phá»¥ thuá»™c vÃ o cÃ¡c Ä‘Æ¡n vá»‹ khÃ¡c nhau, thay vÃ¬ phá»¥ thuá»™c vÃ o má»™t sá»‘ Ä‘Æ¡n vá»‹ cá»¥ thá»ƒ. VÃ­ dá»¥, náº¿u chÃºng ta cÃ³ má»™t máº¡ng nÆ¡-ron cÃ³ 100 Ä‘Æ¡n vá»‹, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng dropout vá»›i tá»· lá»‡ 20%. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  trong má»—i láº§n cáº­p nháº­t tham sá»‘, chÃºng ta sáº½ ngáº«u nhiÃªn loáº¡i bá» 20% cÃ¡c Ä‘Æ¡n vá»‹ trong máº¡ng. Láº¥y vÃ­ dá»¥ trong hÃ¬nh áº£nh sau: 

![Dropout](/assets/img/blog1/dropout.png)

Ok, nhÆ° váº­y chÃºng ta Ä‘Ã£ hiá»ƒu sÆ¡ sÆ¡ vá» cÃ¡c phÆ°Æ¡ng phÃ¡p kiá»ƒm soÃ¡t mÃ  nhÃ³m tÃ¡c giáº£ sá»­ dá»¥ng, giá» chÃºng ta sáº½ tÃ¬m hiá»ƒu rÃµ hÆ¡n cÃ¡ch mÃ  cÃ¡c tÃ¡c giáº£ lÃ m Ä‘iá»u Ä‘Ã³. Má»i ngÆ°á»i nhá»› láº¡i vector Ä‘áº¡i diá»‡n cuá»‘i cÃ¹ng Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ pháº§n Perceptron Ä‘a táº§ng. MÃ¬nh Ä‘Ã£ nÃ³i lÃ  nÃ³ sáº½ Ä‘i qua má»™t cÃ¡i FCL, vÃ  Ã¡p dá»¥ng hÃ m phi tuyáº¿n sigmoid, nhÆ° váº­y cÃ´ng thá»©c toÃ¡n há»c cá»§a nÃ³ lÃ : 

$$
y = \mathbf{w} \cdot \mathbf{z} + b
$$

Trong Ä‘Ã³ $\mathbf{z}$ lÃ  má»™t vector gá»“m m pháº§n tá»­ vá»›i m lÃ  sá»‘ lÆ°á»£ng filter mÃ  chÃºng ta xÃ i. Do Ä‘Ã³ mÃ  vector nÃ y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a: $\mathbf{z} \in \mathbb{R}^m$. NhÆ°ng nhÆ° Ä‘Ã£ Ä‘á» cáº­p, chÃºng ta cÃ³ sá»­ dá»¥ng dropout, do Ä‘Ã³ mÃ  chÃºng ta cáº§n pháº£i mask (che) Ä‘i vÃ i Ä‘Æ¡n vá»‹ áº©n. Biá»ƒu diá»…n Ä‘Æ¡n giáº£n dÆ°á»›i cÃ´ng thá»©c toÃ¡n há»c, lÃºc nÃ y $y$ sáº½ lÃ :

$$
y = \mathbf{w} \cdot (\mathbf{r} \odot \mathbf{z}) + b
$$

á» cÃ´ng thá»©c trÃªn, $\mathbf{r}$ lÃ  má»™t vector cÃ³ m-chiá»u vá»›i m á»Ÿ Ä‘Ã¢y cÅ©ng chÃ­nh lÃ  sá»‘ lÆ°á»£ng filter mÃ  mÃ´ hÃ¬nh sá»­ dá»¥ng vÃ  Ä‘Æ°á»£c biá»ƒu diá»…n cá»¥ thá»ƒ dÆ°á»›i dáº¡ng toÃ¡n há»c nhÆ° sau: $\mathbf{r} \in \mathbb{R}^m$ . VÃ  cÃ¡c pháº§n tá»­ cá»§a vector nÃ y cÃ³ thá»ƒ xem lÃ  cÃ¡c biáº¿n ngáº«u nhiÃªn tuÃ¢n theo phÃ¢n phá»‘i Bernoulli vá»›i xÃ¡c suáº¥t p báº±ng 1. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh (cá»¥ thá»ƒ á»Ÿ bÆ°á»›c lan truyá»n ngÆ°á»£c), chÃºng ta chá»‰ Ä‘á»¥ng tá»›i nhá»¯ng Ä‘Æ¡n vá»‹ khÃ´ng bá»‹ mask. Trong khi Ä‘Ã³ trong quÃ¡ trÃ¬nh sá»­ dá»¥ng mÃ´ hÃ¬nh, cÃ¡c vector trá»ng sá»‘ $\mathbf{w}$ sáº½ nhÃ¢n thÃªm bá»Ÿi p sao cho $\hat{\mathbf{w}} = p\mathbf{w}$.

NgoÃ i ra nhÃ³m tÃ¡c giáº£ cÅ©ng táº¡o ra rÃ ng buá»™c vá» chuáº©n l2 cá»§a vector trá»ng sá»‘ sá»­ dá»¥ng l2 regularization, cá»¥ thá»ƒ hÆ¡n thÃ¬ há» Ä‘Ã£ thay Ä‘á»•i giÃ¡ trá»‹ cá»§a $\mathbf{w}$ sao cho $\|\mathbf{w}\|_2 = s$ náº¿u nhÆ° $\|\mathbf{w}\|_2 > s$ sau bÆ°á»›c gradient descent. 

## Tá»•ng káº¿t láº¡i.
Äáº§u tiÃªn cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n word2vec Ä‘á»ƒ láº¥y ra cÃ¡c Ä‘áº·c trÆ°ng tá»«. Sau Ä‘Ã³ há» sá»­ dá»¥ng cÃ¡c filter cÃ³ Ä‘á»™ rá»™ng khÃ¡c nhau vá»›i má»¥c Ä‘Ã­ch mong muá»‘n mÃ´ hÃ¬nh náº¯m báº¯t Ä‘Æ°á»£c nhiá»u ngá»¯ cáº£nh trong cÃ¢u hÆ¡n. Sau khi Ã¡p dá»¥ng phÃ©p convolution giá»¯a input Ä‘áº§u vÃ o vá»›i cÃ¡c filter khÃ¡c nhau, há» thu Ä‘Æ°á»£c cÃ¡c feature map khÃ¡c nhau, sau Ä‘Ã³ tiáº¿n hÃ nh max pooling Ä‘á»ƒ láº¥y ra feature mang tÃ­nh Ä‘áº¡i diá»‡n nháº¥t cho cÃ¢u á»©ng vá»›i tá»«ng filter. CÃ¡c units Ä‘Ã³ sáº½ Ä‘Æ°á»£c ná»‘i vá»›i nhau báº±ng phÃ©p ghÃ©p Ä‘á»ƒ lÃ m input cho má»™t máº¡ng MLP vá»›i hÃ m phi tuyáº¿n lÃ  hÃ m softmax Ä‘á»ƒ Ä‘Æ°a Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh thÃ nh cÃ¡c giÃ¡ trá»‹ xÃ¡c suáº¥t. CÃ²n vá» cÆ¡ cháº¿ kiá»ƒm soÃ¡t, há» Ã¡p dá»¥ng dropout cho máº¡ng MLP vÃ  l2 regularization káº¿t há»£p vá»›i objective function.  

DÆ°á»›i Ä‘Ã¢y lÃ  pseudocode mÃ¬nh lÃ m, má»i ngÆ°á»i cÃ³ thá»ƒ tham kháº£o thÃªm nha.

![pseudocode](/assets/img/blog1/textcnnpseudocode.png)

# á»¨ng dá»¥ng
NhÆ° váº­y lÃ  giá» má»i ngÆ°á»i Ä‘Ã£ hiá»ƒu cÃ¡ch mÃ  mÃ´ hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng. Váº­y giá» chÃºng ta sáº½ Ä‘Æ°a nÃ³ vÃ o thá»±c tiá»…n Ä‘á»ƒ xem Ä‘á»‘i vá»›i cÃ¡c bÃ i toÃ¡n NLP (cá»¥ thá»ƒ lÃ  bÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc) thÃ¬ mÃ´ hÃ¬nh cá»§a chÃºng ta hoáº¡t Ä‘á»™ng ra sao. Má»i ngÆ°á»i cÃ³ thá»ƒ báº¥m vÃ o [Ä‘Ã¢y](https://colab.research.google.com/drive/1lDoW_WrXkMpo9ifYAxKwStkky9lVdTct?usp=sharing) Ä‘á»ƒ truy cáº­p vÃ o code cá»§a á»©ng dá»¥ng nÃ y.

**!!clarify!!**: Pháº§n á»©ng dá»¥ng nÃ y sáº½ khÃ´ng sá»­ dá»¥ng cÃ¡c vector chá»¯ Ä‘Æ°á»£c láº¥y tá»« mÃ´ hÃ¬nh ngÃ´n ngá»¯ há»c khÃ´ng giÃ¡m sÃ¡t nhÆ° nhá»¯ng gÃ¬ mÃ  chÃºng ta bÃ n luáº­n á»Ÿ trÃªn, láº§n nÃ y chÃºng ta sáº½ tháº­t sá»± láº¥y ngáº«u nhiÃªn cÃ¡c giÃ¡ trá»‹ trong má»—i pháº§n tá»­ cá»§a vector rá»“i cÃ¡c giÃ¡ trá»‹ nÃ y sáº½ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh thÃ´ng qua quÃ¡ trÃ¬nh huáº¥n luyá»‡n... NÃªn lÃ , Ä‘Ãºng váº­y, lÃ m láº¡i tá»« Ä‘áº§u tá»›i Ä‘uÃ´i luÃ´n ğŸ’©

Vá» bá»™ dá»¯ liá»‡u, thÃ¬ mÃ¬nh sá»­ dá»¥ng má»™t pháº§n cá»§a bá»™ dá»¯ liá»‡u [Amazon Reviews for Sentiment Analysis](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews). Cá»¥ thá»ƒ hÆ¡n thÃ¬ mÃ¬nh chá»‰ láº¥y 40.000 báº£n ghi cho táº­p huáº¥n luyá»‡n vÃ  Ä‘Ã¢u Ä‘Ã³ khoáº£ng 1000 báº£n ghi cho táº­p kiá»ƒm tra. LÆ°u Ã½ á»Ÿ Ä‘Ã¢y lÃ  mÃ¬nh chá»‰ láº¥y máº«u thÃ´i nha, chá»© náº¿u má»i ngÆ°á»i táº£i bá»™ Ä‘Ã³ vá» mÃ  cháº¡y trÃªn mÃ¡y local mÃ  yáº¿u yáº¿u hoáº·c sá»­ dá»¥ng google colab báº£n bÃ¬nh thÆ°á»ng thÃ¬ ná»™i viá»‡c lÆ°u data trong bá»™ nhá»› cÅ©ng ráº¥t tá»‘n kÃ©m Ã¡. BÃªn cáº¡nh Ä‘Ã³ thÃ¬ bá»™ dá»¯ liá»‡u nÃ y Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ­ tá»« trÆ°á»›c rá»“i mÃ¬nh má»›i Ä‘áº©y lÃªn drive (cÃ¡c bÆ°á»›c xá»­ lÃ­ nhÆ° loáº¡i bá» nhÃ£n xáº¥u, v.v... cÅ©ng cÃ³ nhiá»u hÆ°á»›ng tiáº¿p cáº­n má»›i máº» mÃ  cÃ³ dá»‹p thÃ¬ mÃ¬nh sáº½ chia sáº» thÃªm ). VÃ  Ä‘á»ƒ cho Ä‘Æ¡n giáº£n hÆ¡n thÃ¬ bá»™ dá»¯ liá»‡u nÃ y Ä‘ang lÃ  bá»™ dá»¯ liá»‡u cÃ¢n báº±ng (tá»· lá»‡ nhÃ£n cá»§a 2 lá»›p lÃ  báº±ng nhau trÃªn cáº£ táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm thá»­). NhÆ° 2 hÃ¬nh dÆ°á»›i Ä‘Ã¢y:

![Train distribution](/assets/img/blog1/eda_1.png)

![Text distribution](/assets/img/blog1/eda_2.png)

Vá» mÃ´ hÃ¬nh thÃ¬ mÃ¬nh tham kháº£o trong [repository nÃ y](https://github.com/gaopinghai/textCNN_pytorch). Repository nÃ y Ä‘Æ°á»£c cáº¥u trÃºc dá»… Ä‘á»c, dá»… náº¯m báº¯t, náº¿u cÃ³ thá»i gian thÃ¬ má»i ngÆ°á»i nÃªn clone vá» mÃ¡y xong cháº¡y local cÅ©ng ok, nÃ³i chung lÃ  check it out and give the author a star !!!. DÆ°á»›i Ä‘Ã¢y lÃ  toÃ n bá»™ mÃ´ hÃ¬nh:
```python
class textCNN(nn.Module):
    def __init__(self, param):
        super(textCNN, self).__init__()
        ci = 1  # input chanel size
        kernel_num = param['kernel_num'] # output chanel size
        kernel_size = param['kernel_size']
        vocab_size = param['vocab_size']
        embed_dim = param['embed_dim']
        dropout = param['dropout']
        class_num = param['class_num']
        self.param = param
        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=1)
        self.conv11 = nn.Conv2d(ci, kernel_num, (kernel_size[0], embed_dim))
        self.conv12 = nn.Conv2d(ci, kernel_num, (kernel_size[1], embed_dim))
        self.conv13 = nn.Conv2d(ci, kernel_num, (kernel_size[2], embed_dim))
        self.dropout = nn.Dropout(dropout)
        self.fc1 = nn.Linear(len(kernel_size) * kernel_num, class_num)

    def init_embed(self, embed_matrix):
        self.embed.weight = nn.Parameter(torch.Tensor(embed_matrix))

    @staticmethod
    def conv_and_pool(x, conv):
        # x: (batch, 1, sentence_length,  )
        x = conv(x)
        # x: (batch, kernel_num, H_out, 1)
        x = F.relu(x.squeeze(3))
        # x: (batch, kernel_num, H_out)
        x = F.max_pool1d(x, x.size(2)).squeeze(2)
        #  (batch, kernel_num)
        return x

    def forward(self, x):
        # x: (batch, sentence_length)
        x = self.embed(x)
        # x: (batch, sentence_length, embed_dim)
        # TODO init embed matrix with pre-trained
        x = x.unsqueeze(1)
        # x: (batch, 1, sentence_length, embed_dim)
        x1 = self.conv_and_pool(x, self.conv11)  # (batch, kernel_num)
        x2 = self.conv_and_pool(x, self.conv12)  # (batch, kernel_num)
        x3 = self.conv_and_pool(x, self.conv13)  # (batch, kernel_num)
        x = torch.cat((x1, x2, x3), 1)  # (batch, 3 * kernel_num)
        x = self.dropout(x)
        logit = F.log_softmax(self.fc1(x), dim=1)
        return logit

    def init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                m.bias.data.zero_()
```
CÃ²n dÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c tham sá»‘ mÃ  má»i ngÆ°á»i cÃ³ thá»ƒ tinh chá»‰nh Ä‘á»ƒ mÃ´ hÃ¬nh Ä‘áº¡t káº¿t quáº£ tá»‘t hÆ¡n:
```python
textCNN_param = {
    'vocab_size': 10000,
    'embed_dim': 256,
    'class_num': 2,
    "kernel_num": 16,
    "kernel_size": [3, 4, 5],
    "dropout": 0.5,
}

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = textCNN(textCNN_param)
model.to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.00005)

num_epochs = 50
save_model = './model'
os.makedirs(save_model, exist_ok = True)
model_name = 'model_TextCNN'
```
VÃ  sau khi cháº¡y mÃ´ hÃ¬nh vá»›i cÃ¡c tham sá»‘ á»Ÿ trÃªn (sá»­ dá»¥ng GPU T4 free cá»§a google colab ğŸ˜âœŒï¸) thÃ¬ cÃ³ Ä‘Æ°á»£c káº¿t quáº£ nhÆ° hÃ¬nh dÆ°á»›i Ä‘Ã¢y (mÃ´ hÃ¬nh nÃ y cháº¡y cÅ©ng nhanh láº¯m Ã¡ má»i ngÆ°á»i, 50 epochs train vÃ¨o 1 phÃ¡t cá»¡ 5-10 phÃºt lÃ  xong rá»“i).

![Model Result](/assets/img/blog1/model_result.png)

Má»™t Ä‘iá»ƒm Ä‘Ã¡ng lÆ°u Ã½ lÃ  cÃ¡c siÃªu tham sá»‘ mÃ¬nh chá»n lÃ  mÃ¬nh chá»n ngáº«u nhiÃªn thÃ´i, nhÆ°ng káº¿t quáº£ cá»§a mÃ´ hÃ¬nh thÃ¬ váº«n ráº¥t tá»‘t. Tuy nhiÃªn tá»« epoch thá»© 30 trá»Ÿ Ä‘i lÃ  cÃ¡i Ä‘Æ°á»ng mÃ u cam báº¯t Ä‘áº§u Ä‘i ngang rá»“i (nÃ³i dá»… hiá»ƒu lÃ  mÃ´ hÃ¬nh chá»‰ lÃ m Ä‘Æ°á»£c tá»›i Ä‘Ã³ thÃ´i), nÃªn Ä‘á»ƒ trÃ¡nh lÃ£ng phÃ­ tÃ i nguyÃªn tÃ­nh toÃ¡n thÃ¬ má»i ngÆ°á»i nÃªn thá»­ thÃªm cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° [early stopping](https://machinelearningcoban.com/2017/03/04/overfitting/) Ä‘á»ƒ tá»‘i Æ°u chi phÃ­ huáº¥n luyá»‡n mÃ´ hÃ¬nh. 


# Æ¯u vÃ  nhÆ°á»£c Ä‘iá»ƒm
Sau khi nghiÃªn cá»©u cÅ©ng nhÆ° á»©ng dá»¥ng mÃ´ hÃ¬nh cho má»™t bá»™ dá»¯ liá»‡u thá»±c táº¿ thÃ¬ mÃ¬nh sáº½ cÃ¹ng bÃ n luáº­n vá» Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a mÃ´ hÃ¬nh nÃ y.

* Æ¯u Ä‘iá»ƒm:
    * Chi phÃ­ tÃ­nh toÃ¡n tháº¥p: CÃ¡c phÃ©p tÃ­ch cháº­p trong cÃ¡c máº¡ng CNN Ä‘Æ°á»£c Æ°a chuá»™ng má»™t pháº§n bá»Ÿi vÃ¬ chÃºng cÃ³ Ã­t tham sá»‘. 
    * Xá»­ lÃ­ tá»‘t cÃ¡c vÃ¹ng cá»¥c bá»™: CÃ¡c vÃ¹ng cá»¥c bá»™ á»Ÿ Ä‘Ã¢y cÃ³ thá»ƒ hiá»ƒu lÃ  cÃ¡c chá»¯ náº±m gáº§n gáº§n nhau nhÆ° lÃ  má»™t Ä‘oáº¡n ngáº¯n hoáº·c lÃ  n-grams. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi phÃ©p tÃ­ch cháº­p, phÃ©p mÃ  Ã¡p má»™t cÃ¡i filter vÃ o má»™t Ä‘oáº¡n cÃ¡c chá»¯ gáº§n nhau Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c cáº¥u trÃºc cá»¥c bá»™ cÃ³ Ã½ nghÄ©a. 
    

* NhÆ°á»£c Ä‘iá»ƒm:
    * KÃ©m trong viá»‡c náº¯m báº¯t thÃ´ng tin toÃ n cá»¥c (global structure): KhÃ´ng nhÆ° khi mÃ¬nh xá»­ lÃ­ dá»¯ liá»‡u dáº¡ng áº£nh thÃ¬ cÃ¡c cáº¥u trÃºc cá»¥c bá»™ ráº¥t quan trá»ng vÃ  cÃ³ sá»± liÃªn quan cháº·t cháº½ vá»›i nhau. Äiá»u nÃ y cÅ©ng lÃ  Ä‘Ãºng khi chÃºng ta xÃ©t trong ngá»¯ cáº£nh lÃ  cÃ¢u tá»«. Tuy nhiÃªn cÅ©ng cÃ³ ráº¥t nhiá»u trÆ°á»ng há»£p mÃ  cÃ¡c tá»« dÃ¹ náº±m xa nhau (hay nÃ³i cÃ¡ch khÃ¡c lÃ  chÃºng khÃ´ng náº±m gáº§n nhau trong má»™t vÃ¹ng lÃ¢n cáº­n) váº«n gÃ¢y áº£nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n Ã½ nghÄ©a cá»§a cÃ¢u. ÄÃ¢y lÃ  Ä‘iá»u khiáº¿n cho mÃ´ hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng khÃ´ng tá»‘t náº¿u nhÆ° chÃºng ta cÃ³ nhá»¯ng cÃ¢u quÃ¡ dÃ i mÃ  nhá»¯ng chá»¯ dÃ¹ náº±m xa nhau váº«n gÃ¢y áº£nh hÆ°á»Ÿng lÃªn nhau. 

Vá» sÆ¡ bá»™ thÃ¬ Ä‘Ã³ lÃ  nhá»¯ng Æ°u nhÆ°á»£c Ä‘iá»ƒm mÃ  mÃ¬nh tháº¥y trong thuáº­t toÃ¡n nÃ y, má»i ngÆ°á»i cÃ³ thá»ƒ nghiÃªn cá»©u thÃªm Ä‘á»ƒ cÃ¹ng bÃ n luáº­n vá» váº¥n Ä‘á» nÃ y trong tÆ°Æ¡ng lai. 

# Tháº£o luáº­n thÃªm
## Trong pháº¡m vi bÃ i nghiÃªn cá»©u.
Trong pháº¡m vi cá»§a bÃ i nghiÃªn cá»©u, nhÃ³m tÃ¡c giáº£ cÃ³ Ä‘Æ°a ra vÃ i má»™t vÃ i tháº£o luáº­n chÃ­nh mÃ  mÃ¬nh sáº½ Ä‘á» cáº­p á»Ÿ pháº§n dÆ°á»›i Ä‘Ã¢y:
1. **Multichannel vs Single Channel Models**: NhÃ³m tÃ¡c giáº£ tin ráº±ng náº¿u chÃºng ta sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh nhiá»u kÃªnh hÆ¡n thÃ¬ sáº½ trÃ¡nh tÃ¬nh tráº¡ng mÃ´ hÃ¬nh gáº·p váº¥n Ä‘á» overfitting, do Ä‘Ã³ mÃ  hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n mÃ´ hÃ¬nh Ä‘Æ¡n kÃªnh, vÃ  Ä‘iá»u nÃ y sáº½ cÃ ng Ä‘Æ°á»£c thá»ƒ hiá»‡n rÃµ náº¿u nhÆ° chÃºng ta cho mÃ´ hÃ¬nh cháº¡y trÃªn má»™t táº­p dá»¯ liá»‡u nhá». Tuy nhiÃªn káº¿t quáº£ cá»§a mÃ´ hÃ¬nh thÃ¬ láº¡i khÃ´ng nhÆ° mong muá»‘n (cá»¥ thá»ƒ hÆ¡n lÃ  nÃ³ bá»‹ láº«n lá»™n, do Ä‘Ã³ mÃ  **khÃ´ng Ä‘i tá»›i káº¿t luáº­n cÃ¡i nÃ o tá»‘t hÆ¡n Ä‘Æ°á»£c**). Do Ä‘Ã³ mÃ  má»™t hÆ°á»›ng tiáº¿p cáº­n cÃ³ váº» há»£p lÃ­ hÆ¡n lÃ  táº­p trung kiá»ƒm soÃ¡t quÃ¡ trÃ¬nh fine-tuning. Giá» láº¥y vÃ­ dá»¥ nhÆ° thay vÃ¬ mÃ¬nh sá»­ dá»¥ng 2 kÃªnh Ä‘áº§u vÃ o, má»—i kÃªnh cÃ³ kÃ­ch thÆ°á»›c lÃ  `n x k`, vÃ­ dá»¥ nhÆ° `10 x 128`, thÃ¬ thay vÃ¬ sá»­ dá»¥ng 2 kÃªnh nhÆ° váº­y, mÃ¬nh chá»‰ cáº§n sá»­ dá»¥ng 1 kÃªnh thÃ´i vÃ  kÃªnh Ä‘Ã³ sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t mÃ  tráº­n cÃ³ kÃ­ch thÆ°á»›c khÃ¡c lá»›n hÆ¡n, vÃ­ dá»¥ nhÆ° `10 x 256`.

2. **Static vs Non-static representation**: Má»™t Ä‘iá»ƒm chung thÃº vá»‹ cá»§a viá»‡c chÃºng ta sá»­ dá»¥ng multichannel model (mÃ´ hÃ¬nh Ä‘a kÃªnh) vÃ  singlechannel model (mÃ´ hÃ¬nh Ä‘Æ¡n kÃªnh) Ä‘Ã³ chÃ­nh lÃ  viá»‡c chÃºng Ä‘á»u **cÃ³ thá»ƒ tinh chá»‰nh non-static channel (cÃ¡c kÃªnh khÃ´ng tÄ©nh) sao cho nÃ³ hoáº¡t Ä‘á»™ng tá»‘t nháº¥t á»Ÿ cÃ¡i task mÃ  chÃºng ta giao** cho nÃ³. VÃ  nhÃ¢n Ä‘Ã¢y, má»™t Ä‘iá»u thÃº vá»‹ khÃ¡c mÃ  mÃ¬nh muá»‘n Ä‘á» cáº­p Ä‘áº¿n chÃ­nh lÃ  vá» viá»‡c biá»ƒu diá»…n tá»« dÆ°á»›i dáº¡ng vector. ChÃºng ta cÃ³ nháº­n xÃ©t ráº±ng má»™t biá»ƒu diá»…n tá»« dÆ°á»›i dáº¡ng vector tá»‘t lÃ  má»™t biá»ƒu diá»…n thá»±c táº¿ cá»§a tá»« ngoÃ i Ä‘á»i sá»‘ng. Tuy nhiÃªn má»™t mÃ´ hÃ¬nh nÃ o Ä‘Ã³ thÃ¬ sáº½ chá»‰ nháº­n má»™t táº­p dá»¯ liá»‡u huáº¥n luyá»‡n nÃ o Ä‘Ã³ thÃ´i, vÃ  khÃ¡ cháº¯c cháº¯n lÃ  táº­p huáº¥n luyá»‡n Ä‘Ã³ sáº½ khÃ´ng thá»ƒ nÃ o pháº£n Ã¡nh táº¥t cáº£ cÃ¡c Ã½ nghÄ©a cá»§a tá»« ngoÃ i Ä‘á»i tháº­t. Do Ä‘Ã³ mÃ  dá»¯ liá»‡u Ä‘áº§u vÃ o cÅ©ng ráº¥t quan trá»ng, **náº¿u ta cáº§n cÃ¡c vector tá»« biá»ƒu diá»…n tá»‘t cÃ´ng viá»‡c mÃ  chÃºng ta giao thÃ¬ chÃºng ta cÅ©ng cáº§n Ä‘áº£m báº£o ráº±ng dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  hoÃ n toÃ n phÃ¹ há»£p cho mÃ´ hÃ¬nh Ä‘á»ƒ há»c**. 

## NgoÃ i pháº¡m vi bÃ i nghiÃªn cá»©u.
KhÃ´ng chá»‰ trong khuÃ´n khá»• bÃ i nÃ y mÃ  ta tháº¥y Ä‘Æ°á»£c sá»©c máº¡nh cá»§a CNN trong viá»‡c xá»­ lÃ­ dá»¯ liá»‡u vÄƒn báº£n. CÃ³ má»™t vÃ i bÃ i nghiÃªn cá»© khÃ¡c cÅ©ng Ä‘Ã£ Ä‘Ã o sÃ¢u hÆ¡n, cho ta tháº¥y thÃªm nhiá»u gÃ³c nhÃ¬n má»›i cá»§a mÃ´ hÃ¬nh nÃ y nhÆ° bÃ i ["What Does a TextCNN Learn?"](https://arxiv.org/pdf/1801.06287.pdf). Trong bÃ i nÃ y, nhÃ³m tÃ¡c giáº£ Ä‘Æ°a ra cÃ¡c káº¿t luáº­n nhÆ° sau:
* CÃ¡c filter há»c cÃ¡c Ä‘áº·c trÆ°ng vá» nhÃ£n: CÃ¡c filter trong TextCNN cÃ³ kháº£ nÄƒng há»c cÃ¡c Ä‘áº·c Ä‘iá»ƒm liÃªn quan Ä‘áº¿n cÃ¡c nhÃ£n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i vÄƒn báº£n.
* Má»™t sá»‘ filter cÃ³ tÃ­nh tÆ°Æ¡ng Ä‘á»“ng: Má»™t sá»‘ filter trong máº¡ng cÃ³ thá»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng tÆ°Æ¡ng tá»± nhau, cho tháº¥y sá»± trÃ¹ng láº·p trong viá»‡c trÃ­ch xuáº¥t thÃ´ng tin.
* Má»™t sá»‘ filter há»c cÃ¡c Ä‘áº·c trÆ°ng chung cá»§a cÃ¡c lá»›p khÃ¡c nhau: CÃ¡c filter cÅ©ng cÃ³ thá»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng chung cho cÃ¡c lá»›p khÃ¡c nhau, giÃºp mÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.
* Äá»™ sÃ¢u cá»§a lá»›p áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c há»c: Sá»‘ lÆ°á»£ng lá»›p trong máº¡ng TextCNN cÃ³ tÃ¡c Ä‘á»™ng Ä‘áº¿n má»©c Ä‘á»™ phá»©c táº¡p vÃ  sá»± trá»«u tÆ°á»£ng cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c há»c.

NgoÃ i ra cÅ©ng cÃ³ má»™t vÃ i bÃ i cáº£i thiá»‡n khÃ¡c nháº±m tÄƒng Ä‘á»™ chÃ­nh xÃ¡c cá»§a thuáº­t toÃ¡n nhÆ° bÃ i ["A combination of TEXTCNN model and Bayesian classifier for microblog sentiment analysis"](https://link.springer.com/article/10.1007/s10878-023-01038-1). Trong bÃ i nÃ y, cÃ¡c tÃ¡c giáº£ khÃ´ng sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p embedding truyá»n thá»‘ng, thay vÃ o Ä‘Ã³ sá»­ dá»¥ng mÃ´ hÃ¬nh [ELMo](https://arxiv.org/pdf/1802.05365.pdf) Ä‘á»ƒ táº¡o cÃ¡c vector tá»« vÃ  sá»­ dá»¥ng máº¡ng LSTM 2 chiá»u Ä‘á»ƒ há»c Ä‘áº·c trÆ°ng tá»«. CÃ²n vá» cáº¥u trÃºc mÃ´ hÃ¬nh, bÃªn cáº¡nh nhá»¯ng cÃ¡i thay Ä‘á»•i vá» lá»›p embedding nhÆ° vá»«a nÃªu, thÃ¬ ngÆ°á»i ta chá»‰ Ä‘á»•i cÃ¡i lá»›p fully connected sang thÃ nh cÃ¡i Naive Bayes Classifier. 

SÆ¡ sÆ¡ thÃ¬ Ä‘Ã³ lÃ  má»™t vÃ i viá»‡c mÃ  ngÆ°á»i ta Ä‘Ã£ lÃ m Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  cáº£i thiá»‡n káº¿t quáº£ cá»§a thuáº­t toÃ¡n, cÃ³ thá»ƒ cÃ³ thÃªm nhiá»u mÃ´ hÃ¬nh khÃ¡c cÅ©ng hay ho mÃ  mÃ¬nh chÆ°a biáº¿t, nÃªn náº¿u cÃ³ dá»‹p thÃ¬ mong Ä‘Æ°á»£c chia sáº» thÃªm vá»›i má»i ngÆ°á»i ğŸ˜»

# Trao Ä‘á»•i thÃªm.
Náº¿u cÃ³ khÃºc máº¯c hay cáº§n tháº£o luáº­n thÃªm vá» váº¥n Ä‘á» gÃ¬ thÃ¬ má»i ngÆ°á»i cÃ³ thá»ƒ liÃªn há»‡ vá»›i mÃ¬nh qua email: [tommyquanglowkey2011@gmail.com](tommyquanglowkey2011@gmail.com)

**CheersğŸ¥‚**

# References

1. ["Convolutional Neural Network for Sentence Classification"](https://arxiv.org/pdf/1408.5882.pdf)

2. ["The Secret to Improved NLP: An In-Depth Look at the nn.Embedding Layer in PyTorch"](https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16)

3. ["Intuitively Understanding Convolutions for Deep Learning"](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)

4. [" C4W1L09 Pooling Layers "](https://www.youtube.com/watch?v=8oOgPUO-TBY)

5. ["Linear layers explained in a simple way"](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjTopqI8duDAxX7k1YBHUl3BdYQFnoECAoQAQ&url=https%3A%2F%2Fmedium.com%2Fdatathings%2Flinear-layers-explained-in-a-simple-way-2319a9c2d1aa&usg=AOvVaw1vjNqtJwb0tZp_vA196sc4&opi=89978449)

6. ["A Convolutional Neural Network for Modelling Sentences"](https://arxiv.org/pdf/1404.2188.pdf)

7. ["Natural Language Processing (almost) from Scratch"](https://arxiv.org/pdf/1103.0398.pdf)

8. ["Efficient Estimation of Word Representations in Vector Space"](https://arxiv.org/pdf/1301.3781.pdf)

9. ["Why We Will Never Open Deep Learningâ€™s Black Box"](https://towardsdatascience.com/why-we-will-never-open-deep-learnings-black-box-4c27cd335118)

10. ["Deep Learning"](https://imlab.postech.ac.kr/dkim/class/csed514_2019s/DeepLearningBook.pdf)

11. ["Amazon Reviews for Sentiment Analysis"](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews)

12. ["textCNN_pytorch"](https://github.com/gaopinghai/textCNN_pytorch)

13. ["BÃ i 15: Overfitting"](https://machinelearningcoban.com/2017/03/04/overfitting/)

14. ["What Does a TextCNN Learn?"](https://arxiv.org/pdf/1801.06287.pdf)

15. ["A combination of TEXTCNN model and Bayesian classifier for microblog sentiment analysis"](https://link.springer.com/article/10.1007/s10878-023-01038-1)

